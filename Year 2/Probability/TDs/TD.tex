\documentclass[10pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Marco Biroli}
\title{TD-Probability}
\usepackage{stmaryrd}

\newcommand{\card}{\text{card}}

\begin{document}
\maketitle
\chapter{TD 1}
\section{A strategic choice.}
Let $X \in \{0, 1\}^3$ (resp. $Y$) be the random variable corresponding to the results of the matches using the first strategy (resp. the second strategy). Then we have that (let $D = \{ (1, 1, 1), (1, 1, 0), (0, 1, 1) \}$):
\[
P(X \in D ) = a^2 b + a b (1-a) + (1-a) b a = ab(2 - a)  
\]  
Similarly:
\[
P(Y \in D) = b^2 a + b a (1-b) + (1-b) a b = ba(2 - b)  
\]
Then since $a > b$ we have that $P(X \in D) < P(Y \in D)$, hence the winning strategy is BAB.

\section{Derangements}
\subsection{}
Let $E$ be a finite set and $A, B \subseteq E$. We denote by $1_A$ the indicator function of $A$ and $\bar{A}$ the complement of $A$. Then we have that:
\[
1_{\bar{A}} = 1 - 1_A \mbox{~~and~~} 1_{A \cap B} = 1_A \cdot 1_B \mbox{~~and~~} 1_{A \cup B} = 1_A + 1_B - 1_{A \cap B}
\]
\subsection{}
We will prove this by induction on $n$. The base case $n = 1$ as well as $n = 2$ are trivially satisfied. Now assume that this is satisfied for $n$ then we have that (using the induction hypothesis for $n = 2$):
\[
\card\left( \bigcup_{i = 1}^n A_i \bigcup A_{n+1} \right) = \card\left( \bigcup_{i = 1}^n A_i  \right) + \card(A_{n+1}) - \card\left( \left(\bigcup_{i = 1}^n A_i\right) \bigcap A_{n+1} \right)
\] 
Now we develop the last term into:
\[
\left(\bigcup_{i = 1}^n A_i\right) \bigcap A_{n+1} = \bigcup_{i = 1}^n \left( A_i \bigcap A_{n+1} \right)
\]
Now applying the induction hypothesis gives the desired result.

\subsection{}
Let $A_i$ be the set of permutations that fixes point $i$. Then from the inclusion-exclusion principle we have:
\[
D_n = n! - \card\left( \bigcup_{i = 1}^n A_i  \right) = n! - \sum_{k = 1}^{n} (-1)^{k-1} \binom{n}{k}(n-k)! = n! \sum_{k = 2}^n \frac{(-1)^k}{k!}
\]

\subsection{}
The probability that no one gets their jacket corresponds to the probability of having a derangement in other words:
\[
p_n = \frac{D_n}{n!} = \sum_{k = 2}^n \frac{(-1)^k}{k!} \stackrel{n\to\infty}{\longrightarrow} \frac{1}{e}
\]

\subsection{}
We have that:
\[
D_{n, l} = \binom{n}{l} D_{n - l} = \binom{n}{l} (n - l)! \sum_{k = 2}^{n - l} \frac{(-1)^k}{k!} = \frac{n!}{l!}\sum_{k = 2}^{n - l} \frac{(-1)^k}{k!}
\]
Hence the probability that eaxctly $l$ people leave with their jackets is:
\[
p_l = \frac{D_{n, l}}{n!} = \frac{1}{l!} \sum_{ k = 2}^{n - l} \frac{(-1)^k}{k!}
\]

\subsection{}
The probability that a given person gets back their jacket is $p_s = \frac{1}{n}$. The probability that at least one person gets back their jacket is:
\[
p_a = 1 - p_n = 1 - \sum_{k = 2}^n \frac{(-1)^k}{k!}
\]
Notice that $p_s < p_a$.

\section{Balls in bins}

\subsection{}
\begin{enumerate}
\item[(a)] If all the balls are distinguishable then we have $\Omega = \llbracket 1 , n \rrbracket^r$ is the set of tuples where each element corresponds to where the $i$-th ball has been sent to. Then $\mathcal{F} = \mathcal{P}(\Omega)$ and since each event is sampled uniformly at random we have that:
\[
\forall \omega \in \Omega, P(\omega) = \frac{1}{|\Omega|} = \frac{1}{n^r}
\] 
Then the probability of $(r_1, \cdots, r_n)$ is given by:
\[
P[(r_1, \cdots, r_n)] = P[\{ \omega \in \Omega : \forall i \in \llbracket 1, n\rrbracket \, \# \{b \in \Omega : b = i\} = r_i \}] = \frac{1}{n^r} \binom{r}{r_1, r_2, \ldots, r_n}
\]
\item[(b)] Now we have that $\Omega = \{ (r_i \in \mathbb{N} : i \in \llbracket 1 , n \rrbracket ) : \sum_{i = 1}^n r_i = r \}$. Again we have that $\mathcal{F} = \mathcal{P}(\Omega)$. Then we have that:
\[
P[(r_1, \ldots, r_n)] = \frac{1}{|\Omega|} = \frac{1}{\binom{r + n - 1}{n - 1}}
\]
\item[(c)] Now we have that $\Omega = \{s \in \{0, 1\}^n : \sum_{i = 1}^n s_i = r \}$ corresponding to the tuple indicating if each state is occupied or not. Once again $\mathcal{F} = \mathcal{P}(\Omega)$. Now the probability is given by:
\[
P[(r_1, \ldots, r_n)] = \frac{1}{|\Omega|} = \frac{1}{\binom{n}{r}}
\]
\end{enumerate}

\subsection{}
The probability that at least two have the same birthday is the 1 minus the probability that none of them share a birthday. The probability that none of them share a birthday is given by $\frac{r!}{n^r}\binom{n}{r}$. Hence the probability that at least two people share a birthday is given by: $1 - \frac{r!}{n^r} \binom{n}{r}$.

\subsection{}
Days are bins, accidents are distinguishable balls hence the probability is given by:
\[
\frac{\binom{r}{n} n^{r - n}}{n^r}  = n^{-n} \binom{r}{n}
\]

\subsection{}


\chapter{TD2}

\section{Symmetric Random Walk}
Consider a balanced coin drawn $n$ times. Denote $X_1, \cdots, X_n$ the results and $S_k$ the partial sums. 
\subsection{}
The law of $S_k$ is given by:
\[
p_{n,r} = P(S_n = r) = \frac{1}{2^n} \binom{n}{\frac{n + r}{2}}
\]

\subsection{}
The number of paths from $(0, 0)$ to $(2n +2, 0)$ never zero are equal to the number of paths from $(1, 1)$ to $(2n + 1, 1)$ which always stay above or equal to the line $y = 1$. Hence rescaling the $y$-axis by a factor 1 we get a bijection in between the strictly positive walks from $(0, 0)$ to $(2n +2, 0)$ with the positive or zero walks from $(0, 0)$ to $(2n, 0)$.
Hence for symmetric random walks the number of random walks going from $(0,0)$ to $(2n+2, 0)$ never touching the axis is twice as much as the number of walks from $(0,0)$ to $(2n, 0)$ being always positive or 0. Furhtemore there are $4$ times more walks going from $0$ to $2n+2$ which therefore gives the desired result. 

\subsection{}
We now that the end of the random walk is going to be given by $a- b$. Now the number of possible only positive walks is given by the number of walks from $(1, 1)$ to $(a+b, a-b)$ minus the number of walks from $(1, -1)$ to $(a+b, a-b)$ by the reflexion principle. Hence we get that:
\[
p = p_{a+b - 1, a-b-1} - p_{a+b - 1, a-b+1} = \frac{1}{2^n}\frac{a-b}{a+b} \binom{a+b}{a}
\] 

\subsection{}

\subsubsection{a)}
Up to a re-scaling of the $y$-axis we have the equivalent problem of computing the number of paths that go from $(0, -r)$ to $(n, k-r)$ and which touch the $x$-axis at least once. Now notice that from the reflexion principle this is equal to the number of paths from $(0, r)$ to $(n, k-r)$ and up to a second shifting this is equal to the number of paths from $(0,0)$ to $(n, k - 2 r)$. Hence the desired probability is given by $p_{n, k - 2r} = p_{n, 2r - k}$. 

\subsubsection{b)}
Then for any $r$ we have that:
\begin{align*}
P(\max\{S_1, \cdots, S_n\} = r) &= \sum_{k = -\infty}^{+\infty} P(S_n = k, \max\{S_1, \cdots, S_n\} = r)\\
&= \sum_{k = -\infty}^{+\infty} ( P(S_n = k, \max\{S_1, \cdots, S_n\} \geq r) - P(S_n = k, \max\{S_1, \cdots, S_n\} > r) )\\
&= \sum_{k = -\infty}^{+\infty} ( P(S_n = k, \max\{S_1, \cdots, S_n\} \geq r) - P(S_n = k, \max\{S_1, \cdots, S_n\} \geq r + 1) )\\
&= \sum_{k = -\infty}^{+\infty}  (p_{n, k - 2r} - p_{n, k - 2r - 2}) = p_{n, r} + p_{n, r+1}
\end{align*}

\subsubsection{c)}
This can be re-written as:
\[
P(S_n = 0, S_1 < 0, \cdots, S_{n-1} < 0, S_0 = -r) = P(S_n = -r, S_1 < 0, \cdots, S_{n-1} < 0, S_0 = 0) 
\]
Now again notice that this corresponds to a symmetric re-writing of the problem 3. Hence we get immediately that:
\[
\hat{p}_{n, r} = P(S_n = r, S_1 < r, \cdots, S_{n-1} < r) = \frac{1}{2^n}\frac{r}{n} \binom{n}{\frac{n + r}{2}} = \frac{r}{n} p_{n, r}
\]
\subsubsection{d)}


\section{Geometric and negative-binomial laws.}
\subsection{}
We want to compute:
\[
P(T_1 = t + 1) 
\]
For such a thing to be the case we need to have thrown tails successively $t$ times and heads the last time. Hence the probability is given by:
\[
P(T_1 - 1 = t) = P(T_1 = t + 1) = (1-p)^t p = \mathcal{G}(p)
\]
The expectancy of $\mathcal{G}(p)$ is given by:
\[
\mathbb{E}[\mathcal{G}(p)] = \sum_{t = 0}^{+\infty} (1 - p)^t p t = p \cdot \frac{1 - p}{p^2} = \frac{1 - p}{p}
\]
The variance of $\mathcal{G}(p)$ is given by:
\[
\text{Var}[\mathcal{G}(p)]^2 = \frac{p^2 - 3p + 2}{p^2} - \frac{1 - 2p + p^2}{p^2} = \frac{1-p}{p^2}
\]

\subsection{}
A geometric law corresponds to something not happening $t$ times and then happening at the $t+1$ time. Then the infimum of two geometric laws corresponds to two things not happening $t$ times and one of them happening at the $t+1$ time. The probability of which is given as follows:
\[
P[(\inf(S_1, S_2) = s] = (1-p)^{2(s - 1)} (p^2 + 2p(1 - p)) = (1-p)^{2(s - 1)} (2p - p^2) = (1-p)^{2(s - 1)} (1 - (1-p)^2) 
\]
Hence the infimum is a geometric variable with law $\mathcal{G}(1 - (1 - p)^2)$

\subsection{}
We want to compute $P(T_m - m = k) = $.

\end{document}