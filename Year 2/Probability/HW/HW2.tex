\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{tikz}
\usepackage{stmaryrd}

\usepackage[left=1cm,right=1cm,top=2cm,bottom=2cm]{geometry}
\author{Marco Biroli}
\title{HW2 - Probability}

\begin{document}
\maketitle

\section{Change of variables}
\begin{enumerate}

\item From the change of variable theorem we know that:
\[
f_{U, V}(u, v) = f_{X, Y}(uv, v(1 - u)) |J|^{-1}
\]
Where:
\[
J = \begin{vmatrix}
\frac{y}{(x+y)^2} & -\frac{x}{(x+y)^2}\\
1 & 1
\end{vmatrix} = \left|\frac{1}{x+y}\right| =  |v^{-1}|
\]
Then replacing in the definition and using the fact that $X$ and $Y$ are independent and hence we can split the joint law we get that:
\begin{align*}
f_{U,V}(u, v) &= \frac{uv^{k-1}}{(k-1)!} e^{-uv} 1_{\mathbb{R}^+}(uv) \frac{v^{k-1}(1 - u)^{k-1}}{(k - 1)!} e^{-v(1- u)} 1_{\mathbb{R}^+} (v(1 - u)) |v|\\
&= \left( \frac{u^{k-1} (1 - u)^{k-1}}{(k-1)!} \right)\left( \frac{v^{2k - 2} |v| e^{-v}}{(k-1)!} \right) 1_{\mathbb{R}^+}(uv) 1_{\mathbb{R}^+}(v(1 - u))
\end{align*}
Now notice that:
\[
\begin{cases}
uv \geq 0\\
v(1 - u) \geq 0
\end{cases}
\Leftrightarrow
\begin{cases}
u, v \geq 0 \lor u, v \leq 0\\
v, (1 - u) \geq 0 \lor v, (1 - u) \leq 0
\end{cases}
\Leftrightarrow
u, v \geq 0
\]
Hence we can rewrite the above as:
\[
f_{U,V}(u, v) = \left( \frac{u^{k-1} (1 - u)^{k-1}}{(k-1)!} 1_{\vb{R}^+}(u) \right)\left( \frac{v^{2k - 1}e^{-v}}{(k-1)!} 1_{\vb{R}^+}(v) \right)
\]
We can already see that $U, V$ are independent. The only thing left to compute is the normalization coefficient of at least one of the two laws. The law of $V$ normalizes obviously to $\Gamma(2k)$ from the definition of the $\Gamma$ function. Hence we can rewrite the above as:
\[
f_{U,V}(u, v) = \left( \frac{u^{k-1} (1 - u)^{k-1} \Gamma(2k)}{(k-1)!^2} 1_{\vb{R}^+}(u) \right)\left( \frac{v^{2k - 1}e^{-v}}{\Gamma(2k)} 1_{\vb{R}^+}(v) \right) = f_U(u) f_V(v)
\]

\item An immediate computation gives:
\[
E[X] = \int_\mathbb{R} t f(t) \dd t = \int_{\mathbb{R}^+} \frac{t^k}{(k-1)!} e^{-t} \dd t = \frac{\Gamma(k+1)}{\Gamma(k)} = k 
\]
Then $X, Y$ are identically distributed hence $E[Y] = E[X]$ and therefore:
\[
E[V] = E[X+Y] = E[X] + E[Y] = 2k
\]
Then we have that $X = UV$ and using the fact that $U, V$ are independent we get:
\[
E[X] = E[U]E[V] \Rightarrow E[U] = \frac{1}{2}
\]

\end{enumerate}

\section{Order statistics}

\begin{enumerate}

\item Let $(\Omega_i, \mathcal{F}_i, P_i)$ be the probability space of $X_i$ then define the product probability space as $(\Omega, \mathcal{F}, P)$ and $X$ as $(X_1, \cdots, X_n)$. Let $(\Omega, \mathcal{F}, P)$ also be the probability space of $T$. Then we define:
\begin{align*}
X_T : \Omega &\longrightarrow \mathbb{R}\\
\vb{x} &\longmapsto X(\vb{x})_{T(\vb{x})}
\end{align*}
Then let $B \in \mathcal{B}(\mathbb{R})$ then we have that:
\begin{align*}
\{\vb{x} \in \Omega : X_T(\vb{x}) \in B\} &\subset \bigotimes_{i \in \llbracket 1, n \rrbracket} \{x_i \in \Omega_i :  X_i(x_i) \in B\} \in \mathcal{F}
\end{align*}
Where the belonging to $\mathcal{F}$ follows from the definition of the product $\sigma$-algebra. 

\item In order to define $(X_{(1)}, \cdots, X_{(n)})$ properly we consider it as an r.v. on the space $(\Omega, \mathcal{F}, P)$ defined as:
\begin{align*}
(X_{(1)}, \cdots, X_{(n)}) : \Omega &\longrightarrow \mathbb{R}^n\\
\vb{x} &\mapsto \sigma{\vb{x}}(X(\vb{x}))
\end{align*}
Where $\sigma_{\vb{x}}$ is the permutation that put $X(\vb{x})$ in increasing order. Since we have a finite list of real numbers we know from the constructions of the real numbers that such a $\sigma_{\vb{x}}$. Furthermore adding as a constraint that in case of parity the smaller index goes first then $\sigma_{\vb{x}}$ is also unique for every $\vb{x}$. Then we have that $\sigma$ is a random variable defined as:
\begin{align*}
\sigma : \Omega &\longrightarrow \mathfrak{S}_n\\
\vb{x} &\longmapsto \sigma_{\vb{x}}
\end{align*}
We furthermore have that $\sigma$ is injective and therefore measurable. Hence $\sigma$ is a well-defined random variable. 

\item From the previous question for shorthand we write $(X_{(1)}, \cdots, X_{(n)}) = \sigma(X)$ as an abuse of notation for: 
\[
(X_{(1)}, \cdots, X_{(n)})(\vb{x}) = \sigma_{\vb{x}}(X(\vb{x}))
\]
Then notice that:
\[
f_{\sigma(X)}(\vb{x}) \dd \vb{x} = \sum_{\mu \in \mathfrak{S}_n} f_X(\mu^{-1}(\vb{x})) \dd \vb{x} = \sum_{\mu \in \mathfrak{S}_n} f_X(\mu(\vb{x})) \dd \vb{x} = \sum_{\mu \in \mathfrak{S}_n} \prod_{i = 1}^n f_{X_i}(\mu(\vb{x})_i) \dd \vb{x}
\]
Where on the last equality we used that the $X_i$ are independent. Then since the $X_i$ are identically distributed we have that $\forall i, \, f_{X_i} = f_{X_1}$. Now since the product commutes we have that the terms inside the sum are all equal up to a permutation of the terms, hence:
\[
\sum_{\mu \in \mathfrak{S}_n} \prod_{i = 1}^n f_{X_i}(\mu(\vb{x})_i) \dd \vb{x} = \sum_{\mu \in \mathfrak{S}_n} \left( \prod_{i = 1}^n f_{X_1}(x_i) \dd x_i \right) = n!\left( \prod_{i = 1}^n f_{X_1}(x_i) \dd x_i \right) = n! f_X(\vb{x'}) 1_{\vb{x'} = \mu(\vb{x})} \dd \vb{x'}
\]
Where we are free to chose any $\mu \in \mathfrak{S}_n$ since the terms in the product commute. If we fix ourselves with the choice $\mu = \sigma$ we get:
\[
\sum_{\mu \in \mathfrak{S}_n} \prod_{i = 1}^n f_{X_i}(\mu(\vb{x})_i) \dd \vb{x} = n! f_X(\sigma(\vb{x})) \dd \vb{x} = n! f_X(\vb{x'}) 1_{\vb{x'} = \sigma(\vb{x'})} \dd \vb{x'}
\]
Call $\mu$ the function that maps $X_1, \cdots, X_n$ to $X_1, \cdots, X_n - X_{n-1}$. Then plugging this in the definition of the expectancy we get:
\begin{align*}
E[\varphi(\mu(\sigma(X)))] &= \int_{\vb{x} \in \Omega} \varphi(\mu(\sigma(X(\vb{x})))) f_{\mu(\sigma(X))}(\mu(\vb{x})) \dd \vb{x} = n! \int_{\vb{x} \in \Omega} \varphi(\mu(\sigma(X(\vb{x})))) f_{\mu(X)}(\mu(\vb{x'})) 1_{\mu(\vb{x'}) = \sigma(\mu(\vb{x'}))} \dd \vb{x}\\
&= n! \int_{\vb{x} \in \Omega} \varphi(\mu(X(\vb{x'}))) f_{\mu(X)}(\mu(\vb{x'})) 1_{\mu(\vb{x'}) = \mu(\sigma(\vb{x'}))} \dd \vb{x'} = n! \mathbb{E}[\varphi(\mu(X)) 1_{\sigma}] \mbox{~~where~~} 1_\sigma(\vb{x}) = \begin{cases}
1 \mbox{~~if~~} \vb{x} = \sigma(\vb{x})\\
0 \mbox{~~otherwise.}
\end{cases}
\end{align*}

\item From the previous exercise and the fact that the $X_{i} - X_{i-1}$ are independent we immediately get that the $X_{(i)} - X_{(i-1)}$ are independent. Then we have that $X_{(1)} = \min_i X_i$ hence:
\[
F_{X_{(1)}}(x) = 1 - \prod_{i = 1}^n P(X_i > x) = 1 - \prod_{i = 1}^n e^{- \alpha x} = 1 - e^{- \alpha n x}
\]
So $X_{(1)}$ follows an exponential law of parameter $n \alpha$. Now consider $X_{(i+1)} - X_{(i)}$. This can be re-written as:
\[
X_{(i+1)} - X_{(i)} = \min_{i \in \llbracket 1, n \rrbracket, X_i > X_{(i)}} X_i - X_{(i)}
\]
However notice that:
\[
P(X_i = x + y | X_i > x) = \frac{P(X_i = x+ y \cap X_i > x)}{P(X_i > x)} = \frac{\alpha e^{-\alpha(x+y)}}{e^{-\alpha x}} = \alpha e^{-\alpha y} = P(X_i = y)
\]
Hence we get that:
\[
X_{(i+1)} - X_{(i)} = \min_{i \in \llbracket 1, n - i\rrbracket} X_i \sim \mbox{Exp}(\alpha(n - i))
\]


\item It is well known that the expectancy of an exponential random variable of parameter $\alpha$ is given by $\frac{1}{\alpha}$. Hence from the previous question we have that:
\[
\mathbb{E}[X_{(i+1)} - X_{(i)}] = \frac{1}{\alpha(n - i)} \mbox{~~and~~} \mathbb{E}[X_{(1)}] = \frac{1}{\alpha n }
\]
Denote by $u_i = \mathbb{E}[X_{(i)}]$ then we have that:
\[
u_1 = \frac{1}{\alpha n} \mbox{~~and~~} u_{i+1} = u_i + \frac{1}{\alpha(n - i)} = \sum_{\ell = 0}^{i} \frac{1}{\alpha(n - \ell)}
\]

\item Notice that:
\[
f_{X_{(k)}} = f_{X_{(1)} + (X_{(2)} - X_{(1)}) + \cdots + (X_{(k)} - X_{(k-1)}} = f_{X_{(1)}} \star f_{X_{(2)} - X_{(1)}} \star \cdots \star f_{X_{(k)} - X_{(k-1)}}
\]
Or in other words if we denote by $(Y_j)$ independent exponential random variables of parameter $\alpha$ we have that:
\[
X_{(k)} = \sum_{i = 1}^{k-1} X_{(i)} - X_{(i-1)} 1_{i > 1} = \sum_{i = 1}^k \frac{Y_i}{n - i + 1}
\]

\item In general we have that:
\[
F_{X_{(k)}}(x) = P\left(\max_{i \in \mathcal{I}} X_i < x \land \min_{i \in \llbracket 1, n \rrbracket \setminus \mathcal{I}} X_i > x \big| |\mathcal{I}| \geq k\right) = \sum_{i = k}^n \binom{n}{i} F_{X_1}(x)^i (1 - F_{X_1}(x))^{n - i}
\]
Where this comes simply from choosing which $i$ elements will be smaller than $x$, and the fact that we need at least $k$ elements to be smaller than $x$. Now simply taking the derivative with respect to $x$ of the previous result we get that:
\[
f_{X_{(k)}}(x) = \dv{}{x} \sum_{i = k}^n \binom{n}{i} F_{X_1}(x)^i (1 - F_{X_1}(x))^{n - i}
\]
Now we denote:
\[
s_i = \dv{}{x} F_{X_1}(x)^i (1 - F_{X_1}(x))^{n - i} = f_{X_1}(x) i F_{X_1}(x)^{i - 1}(1 - F_{X_1}(x))^{n - i} - f_{X_1}(x) (n - i) F_{X_1}(x)^i (1 - F_{X_1}(x))^{n - i - 1} = \ell_i - r_i
\]
And notice that $\ell_{i+1} = r_i \frac{i + 1}{n - i}$. Hence we get:
\[
f_{X_{(k)}}(x) - \binom{n}{k} k f_{X_1}(x) F_{X_1}(x)^{k-1} (1 - F_{X_1}(x))^{n - k} = \sum_{i = k}^{n-1} \binom{n}{i + 1} (i + 1) \ell_{i + 1} - \binom{n}{i} r_i = 0
\]
We therefore get:
\[
f_{X_{(k)}}(x) = \binom{n}{k} k f_{X_1}(x) F_{X_1}(x)^{k-1} (1 - F_{X_1}(x))^{n - k}
\]

\end{enumerate}


\end{document}