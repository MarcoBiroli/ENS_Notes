\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{tikz}
\usepackage{stmaryrd}

\usepackage[left=1cm,right=1cm,top=2cm,bottom=2cm]{geometry}
\author{Marco Biroli}
\title{HW1 - Probability}

\begin{document}
\maketitle

\section{Generating functions.}
\begin{enumerate}

\item We have:
\[
g_X(s) = E[s^X] = \sum_{n \geq 0} P[X = n]s^n \mbox{~~and hence~~} g_X(1) = \sum_{n \in \mathbb{N}} P[X = n] = 1
\]

\item We know that:
\[
|g_X(s)| \leqslant \sum_{n \in \mathbb{N}} s^n
\]
Hence $g_X$ is well defined for all $s \in [0, 1[$. Furthermore:
\[
\left|\dv[k]{g_X(s)}{s}\right| \leqslant \sum_{n \in \mathbb{N}} (n+k)^k s^{n}  
\] 
Hence every derivative also has a radius of convergence of 1. 

\item For a Bernouilli variable we get:
\[
g_X(s) = (1- p)s^0 + p \cdot s = 1 - p + p s
\]

\item Notice that:
\[
P[X = i] = \dv[i]{g_X}{s} \Big|_{s = 0}
\]
Hence the law of $X$ is fully characterized by $g_X$. 

\item We have:
\[
E[X^k] = \sum_{n \in \mathbb{N}} P[X = n] n^k 
\]
And we also have:
\[
\dv[k]{g_X(s)}{s} = \sum_{n \geq 0} P[X = n+k](n + k) \cdots (n - 1) \cdot  s^{n}
\]
Now the radius of convergence of this series is given by:
\begin{align*}
R^{-1} &= \limsup_{n \to +\infty} \sqrt[n]{P[X = n + k](n + k) \cdots (n - 1)} = \limsup_{n \to +\infty} \sqrt[n]{P[X = n + k] (n + k)^k \cdot 1 \cdots \frac{n - 1}{n + k}}  \\
&= \limsup_{n \to +\infty} \sqrt[n]{P[X = n + k](n + k)^k}  = \limsup_{n \to +\infty} \sqrt[n]{P[X = n]n^k} 
\end{align*}
Hence we see that:
\[
f(s) = \sum_{n \in \mathbb{N}} P[X = n]n^k s^n \mbox{~~and~~} \dv[k]{g_X(s)}{s} = \sum_{n \geq 0} P[X = n](n + k) \cdots (n - 1) \cdot  s^{n}
\]
Have the same radius of convergence. Hence $E[X^k] = \lim_{s \to 1^-} f(x)$ converges if and only if $\lim_{s \to 1^-} \dv[k]{g_X(s)}{s}$ converges.

\item We have:
\[
\dv{g_X(s)}{s} = \sum_{n \geqslant 1} P[X = n] n s^{n-1}
\]
Hence we get that $E[X] = \dv{g_X}{s}\Big|_{s = 0}$. Now notice that $V[X] = E[X^2 - 2 E[X] X + E[X]^2] = E[X^2] - 2 E[X]^2 + E[X]^2 = E[X^2] - E[X]^2$. Now notice that:
\[
\dv[2]{g_X(s)}{s} = \sum_{n \geq 2} P[X = n] n (n - 1) s^{n - 2} = \sum_{n \geq 2} (P[X = n]n^2 - P[X = n] n)s^{n - 2} = \sum_{n \geq 0} P[X = n]n^2 - \frac{1}{s}\dv{g_X(s)}{s}
\]
Hence we have that $E[X^2] = \left(\dv[2]{g_X}{s} + \dv{g_X}{s}\right)\Big|_{s = 0}$. Putting everything together gives:
\[
V[X] = \left(\dv[2]{g_X}{s} + \dv{g_X}{s}\right)\Big|_{s = 0} - \left( \dv{g_X}{s}\Big|_{s = 0} \right)^2
\]

\item We have that $g_{S_n}(s) = E[s^{\sum_{i = 1}^n X_i}] = E[\prod_{i = 1}^n s^{X_i}] = \prod_{i = 1}^n E[s^{X_i}] = \prod_{i = 1}^n g_{X_i}(s)$.

\item Applying question 8 with the results of question 2 we know that the generating function of a binomial law of parameters $(n, p)$ represented by the r.v. $Y$ is given by:
\[
g_Y(s) = \prod_{i = 1}^n (1 - p(1 + s)) = (1 - p(1 + s))^n
\]

\item We have that $g_Y(s) = E[s^{\sum_{1 \leq i \leq U} X_i}] = E[\prod_{i = 1}^U s^{X_i}] = E[\prod_{i = 1}^U E[s^{X_i}]] = E[E[s^{X_1}]^U] = g_U \circ g_{X_1} (s)$.

\item See question 2 for the first part of the question. Then we have:
\begin{align*}
\int_0^{2\pi} g_X(e^{i \theta}) e^{- i k \theta} \dd \theta &= \int_{0}^{2 \pi} \sum_{n \in \mathbb{N}} P[X = n] e^{i \theta(n - k)} \dd \theta = \sum_{n \in \mathbb{N}} P[X = n] \int_0^{2 \pi} e^{i \theta ( n -k )} \dd \theta \\
&= \sum_{n \in \mathbb{N}} P[X = n] 2 \pi \delta(n - k) = 2 \pi P[X = k]  
\end{align*}  

\item Applying the inversion formula we have that:
\begin{align*}
\lim_{n \to +\infty} P[X_n = k] &= \lim_{n \to +\infty} \frac{1}{2\pi} \int_0^{2\pi} g_{X_n}(e^{i\theta})e^{-ik\theta} \dd \theta = \frac{1}{2\pi} \int_0^{2\pi} \lim_{n \to +\infty} g_{X_n}(e^{i\theta})e^{- i k \theta} \dd \theta \\
&= \frac{1}{2\pi}\int_0^{2\pi} g_{X}(e^{i\theta})e^{-ik\theta} \dd \theta = P[X = k]
\end{align*}
\end{enumerate}

\section{Simple random walk.}

\textbf{Fundamental Lemma.} We remind here the statement of the Fundamental Lemma seen in the course since it will prove useful multiple times. The Fundamental Lemma states that for a symmetric random walk:
\[
P(S_{2} \neq 0, \cdots, S_{2n} \neq 0) = P(S_{2n} = 0)
\]
Furthermore this follows from the recursive formula given by:
\[
P(S_2 \neq 0, \cdots, S_{2n-2} \neq 0, S_{2n} = 0) = P(S_{2n - 2} = 0) -  P(S_{2n} = 0)
\]
\textbf{Basic probability.} We also remind here the basic probability formula for random walks which follows from combinatorics arguments:
\[
P(S_n = k) = \binom{n}{\frac{n + k}{2}} \frac{1}{2^n}
\]\\

\begin{enumerate}

\item The random walk can reach zero only after an even amount of steps because it must have done $k$ steps up and $k$ steps down for a total of $2k$ steps. Hence $T_{2n}$ must be even.

\item Let $(S_1, \cdots, S_{2n})$ be a path such that $T_{2n} = 2k$ then notice that the unique equivalent path given by $(-S_1, \cdots, -S_{2n})$ has $T_{2n} = 2n - 2k$ hence the conclusion follows.

\item To solve this question we consider an equivalent problem. Notice that number of strictly negative paths from 0 to $2n + 1$ is equal to the number of paths equal or below $-1$ from 1 to $2n + 1$ since the first step must be negative and we then must stay at all time below the $-1$ axis. Then up to a re-scaling of the $y$-axis we know that this is equal to the negative or zero paths of length $2n$ which is what we are looking for. Then we know from the Fundamental Lemma seen in the course that the number of strictly negative paths from $0$ to $2n$ is given by:
\[
\#\{S_1 < 0, \cdots, S_{2n} < 0\} = \frac{1}{2}\#\{S_1 \neq 0, \cdots, S_{2n} \neq 0\} = \frac{1}{2}\#\{S_{2n} = 0\}
\] 
Now to every strictly negative path from $0$ to $2n$ we have $2$ possible paths from $0$ to $2n+1$ since we get an extra 2 choices for the last step. Furthermore one needs not to worry for the last step being zero since it is impossible to return at 0 after an odd number of steps: $2n+1$. Hence we get that:
\[
\#\{ S_1 \leq 0, \cdots, S_{2n} \leq 0 \} = \#\{ S_1 < 0, \cdots, S_{2n + 1} < 0 \} = \frac{1}{2}\#\{S_{2n} = 0\} \cdot 2 = \#\{S_{2n} = 0\}
\]
Hence:
\[
P(T_{2n} = 0) = P(S_1 \leq 0, \cdots, S_{2n} \leq 0) = P(S_{2n} = 0)
\]
\vspace{1cm}
\hrule
\textbf{Intermediary Result.} At the start of question 4 we will first prove the following statement which will turn out to be helpful in question 5. We therefore single it out here for clarity:
\[
\frac{P(S_{2n} = 0)}{2r - 1} = P(S_1 \neq 0, \cdots, S_{2n-1} \neq 0, S_{2n} = 0)
\]
\hrule
\vspace{1cm}
\item Let $(S_1, \cdots, S_{2n})$ be a path such that $T_{2n} = 2k$ since $ 1 \leq k \leq n - 1$ we know that there are at least 2 steps in the negative plane and at most $2n-2$ and hence the path must have at least one return to the origin. Call the time of the first such return $2r$. We now want to compute the following:
\[
P(S_1 > 0, S_2 > 0, \cdots, S_{2r-1} > 0, S_{2r} = 0) = \frac{1}{4} P(S_1 \geq 0, S_2 \geq 0, \cdots, S_{2r - 3} \geq 0, S_{2r - 2} = 0)
\]
Now using the formula from TD2 we get:
\[
P(S_1 > 0, S_2 > 0, \cdots, S_{2r-1} > 0, S_{2r} = 0) = \frac{1}{2} P(S_1 \neq 0, \cdots, S_{2r - 1} \neq 0, S_{2r} = 0)
\]
Now using the formula from the course from which the fundamental lemma is derived we get that:
\[
P(S_1 > 0, S_2 > 0, \cdots, S_{2r-1} > 0, S_{2r} = 0) = 2^{-1} (P(S_{2r  -2} = 0) - P(S_{2r} = 0))
\]
Now replacing these two probabilities with their expression (also derived in the course) we get:
\[
P(S_1 > 0, S_2 > 0, \cdots, S_{2r-1} > 0, S_{2r} = 0) = 2^{-1}\left( \frac{(2r - 2)!}{2^{2r - 2} (r - 1)!^2} - \frac{(2r)!}{2^{2r} r!^2} \right)
\]
Now putting everything on the same denominator gives (which also proves the intermediary result):
\[
P(S_1 > 0, S_2 > 0, \cdots, S_{2r-1} > 0, S_{2r} = 0)  = 2^{-1}\left( \frac{1}{2r - 1} \frac{(2r)!}{2^{2r} r!^2} \right) = \frac{2^{-1}}{2r - 1} P(S_{2r} = 0)
\]
Now there are two possible cases, either the path was positive before the return to 0 and hence we have $2k - 2r$ steps left that need to be positive in the $2n - 2r$ remaining steps. Or the path was negative before the first return to 0 and hence we need $2k$ steps to be positive in the $2n - 2r$ steps left. This leads to the following formula:
\begin{align*}
P(T_{2n} = 2k) = \sum_{r = 1}^k &P(S_1 > 0, \cdots, S_{2r - 1} > 0, S_{2r} = 0) P(T_{2n - 2r} = 2k - 2r) \\
&+ \sum_{r = 1}^{n - k} P(S_1 < 0, \cdots, S_{2r - 1} < 0, S_{2r} = 0) P(T_{2n - 2r} = 2k)\\
= \sum_{r = 1}^k &\frac{2^{-1}}{2r - 1} P(S_{2r} = 0) P(T_{2n - 2r} = 2k - 2r) \\
&+ \sum_{r = 1}^{n - k} P(S_1 < 0, \cdots, S_{2r - 1} < 0, S_{2r} = 0) P(T_{2n - 2r} = 2k)\\
= \sum_{r = 1}^k &\frac{2^{-1}}{2r - 1} P(S_{2r} = 0) P(T_{2n - 2r} = 2k - 2r) \\
&+ \sum_{r = 1}^{n - k} \frac{2^{-1}}{2r - 1} P(S_{2r} = 0) P(T_{2n - 2r} = 2k)\\
\end{align*}
Where in the $3^{rd}$ equality we used the fact that the number of paths strictly above the axis is equal to the number of paths below which follows from a mirror symmetry along the axis. Now multiplying left and right by powers of 2 we get the formula asked for in the homework.

\item Let $\mathcal{H}_n : "  \forall k \in \llbracket 0, n \rrbracket, \quad P(T_{2n} = 2k) = 2^{-2n} \binom{2k}{k} \binom{2n - 2k}{n - k}"$. We have that $\mathcal{H}_0$ is trivially true. Then take $n \in \mathbb{N} $ such that $\mathcal{H}_m$ is true for $m \in \llbracket 0, n - 1 \rrbracket$. As seen in the course we are going to re-write the arcsin law as follows for more compact notation: $P(T_{2n} = 2k) = P(S_{2k} = 0)P(S_{2n - 2k} = 0)$. Then we get that:
\begin{align*}
P(T_{2n} = 2k) &= \sum_{r = 1}^k \frac{P(S_{2r} = 0)}{2(2r - 1)} P(T_{2n - 2r} = 2k - 2r) + \sum_{r = 1}^{n - k} \frac{P(S_{2r} = 0)}{2(2r - 1)} P(T_{2n - 2r} = 2k)\\
&= \sum_{r = 1}^k \frac{P(S_{2r} = 0)}{2(2r - 1)} P(S_{2k - 2r} = 0)P(S_{2n - 2k} = 0) + \sum_{r = 1}^{n - k} \frac{P(S_{2r} = 0)}{2(2r - 1)} P(S_{2k} = 0)P(S_{2n - 2r - 2k} = 0)\\
&= \frac{P(S_{2n - 2k} = 0)}{2}\sum_{r = 1}^k \frac{P(S_{2r} = 0)}{2r - 1} P(S_{2k - 2r} = 0) + \frac{P(S_{2k} = 0)}{2}\sum_{r = 1}^{n - k} \frac{P(S_{2r} = 0)}{2r - 1} P(S_{2n - 2r - 2k} = 0)\\ 
\end{align*}
However now using the intermediary result of question 4 notice that the first sum can be re-written as (and similarly for the second sum):
\[
\sum_{r = 1}^k P(S_1 \neq 0, \cdots, S_{2r - 1} \neq 0, S_{2r} = 0) P(S_{2 k - 2r} = 0)
\]
Notice that every term of the sum corresponds to the probability of the event: $A_r$ : "the walk touches zero for the first time at step $2r$ and finishes at 0 after $2k$ steps in total". Notice furthermore that all the $A_i$ are disjoint since the 'first time' condition necessarily separates them. Hence we can rewrite the sum as:
\[
\sum_{r = 1}^k P(A_r) = P(\bigcup_{i = 1}^k A_r) = P(S_{2k} = 0)
\]
An identical reasoning can be applied to the second sum and hence we obtain:
\begin{align*}
P(T_{2n} = 2k) &= \frac{P(S_{2n - 2k} = 0)P(S_{2k = 0})}{2} + \frac{P(S_{2k} = 0)P(S_{2n - 2k} = 0)}{2}\\
&= P(S_{2n - 2k} = 0) P(S_{2k} = 0)
\end{align*}
And hence $\{\mathcal{H}_i : i \in \llbracket 0, n- 1 \rrbracket\} \Rightarrow \mathcal{H}_n$ and this concludes the proof.

\item \begin{enumerate}
\item The probability that one of the players leads the whole game is given by $P(T_{20} = 20) + P(T_{20} = 0) = 2P(T_{20} = 0) = \frac{46\,189}{131\,072} \approx 0.35$.  
\item The probability of the winner leading at least 16 times during the game is given  by $2(P(T_{20} \in \{20, 18, 16\}) = \frac{22\,451}{32\,768} \approx 0.69$. 
\item The probability that both players lead 10 times is given by $P(T_{20} = 10) = \frac{3\,969}{65\,536} \approx 0.06$. 
\end{enumerate}

\item Following the same reasoning as in question 4 for $P(S_1 \geq 0, \cdots, S_{2r - 1}\geq 0, S_{2r } = 0)$ and replacing $r$ with $n$ we immediately obtain the desired formula which is:
\[
P(T_{2n} = 2n, S_{2n} = 0) = \frac{2}{2n + 1} P(S_{2n + 2} = 0) = \frac{2}{2n + 1} \frac{P(S_{2n} = 0) (2 n + 1)}{2(n + 1)} = \frac{P(S_{2n} = 0)}{n + 1} = P(T_{2n} = 0, S_{2n} = 0)
\]
Where the last equality directly follows from symmetry with the axis.

\item We take the same recursive formula as previously and consider the first term:
\begin{align*}
\frac{2^{-1}}{2r - 1} P(S_{2r} = 0) \frac{P(S_{2n - 2r} = 0)}{n - r + 1} &= 2^{-1} \left( \frac{P(S_{2r} = 0)}{2r - 1} \right)\left( \frac{P(S_{2n - 2r + 2} = 0)}{2n - 2r + 1} \right) 2\\
&= P(S_2 \neq 0, \cdots, S_{2r - 2} \neq 0, S_{2r} = 0) P(S_2 \neq 0, \cdots, S_{2n - 2r} \neq 0, S_{2n - 2r + 2} = 0)\\
&= P(S_2 \neq 0, \cdots, S_{2r - 2} \neq 0, S_{2r} = 0, S_{2r + 2} \neq 0, \cdots, S_{2n} \neq 0, S_{2n + 2} = 0)
\end{align*}
Notice that this corresponds to the probability of the event $A_r:$' a path of length $2n+2$ admits only two zeroes in $2r$ and at the end in $2n + 2$.". Notice furthermore that all the $A_r$ are disjoint events and furthermore from the symmetry which comes from flipping the horizontal axis we have that $A_r \cong A_{n - r}$. Hence we get that:
\begin{align*}
\sum_{r = 1}^k P(A_r) + \sum_{r = 1}^{n - k} P(A_r) &= P(\bigcup_{r = 1}^k A_r) + \sum_{r = 1}^{n - k} P(A_{n - r}) = P(\bigcup_{r = 1}^k A_r) + \sum_{r = k}^{n - 1} P(A_{r})\\
&= P(\bigcup_{r = 1}^k A_r) + P(\bigcup_{r = k}^{n-1} A_{r}) = P(\bigcup_{r = 1}^{n-1} A_r) = P(S_{2n + 2} = 0, \exists r \in \llbracket 1, n - 1\rrbracket S_{2r} = 0)
\end{align*}
Notice that we could conclude here since the value is independent from $k$ we already know that normalization will force the correct result. Now notice that this is simply a re-writing of the lamplighter problem seen in the lecture and from the lecture we know that:
\begin{align*}
\sum_{r = 1}^k P(A_r) + \sum_{r = 1}^{n - k} P(A_r) &= P(\bigcup_{r = 1}^{n-1} A_r) = P(S_{2n + 2} = 0, \exists r \in \llbracket 1, n - 1\rrbracket S_{2r} = 0)\\
&= 2\frac{P(S_{2n + 2} = 0)}{2n + 2 - 1} = 2\frac{P(S_{2n + 2} = 0)}{2n + 1}\\
&= 2\frac{P(S_{2n} = 0) (2n + 1)}{2 (n + 1)} \frac{1}{2n + 1}\\
&= \frac{P(S_{2n} = 0)}{n + 1}
\end{align*}


\item The most striking difference in between the results of question 5 and 8 is that the result in question 8 is independent of that value of $k$ whilst the one of question 5 is not. 


\end{enumerate}

\end{document}