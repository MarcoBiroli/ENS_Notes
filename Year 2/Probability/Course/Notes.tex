\documentclass[10pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{physics}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Marco Biroli}
\title{Probability}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]

\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\begin{document}
\maketitle
\tableofcontents
\chapter{Founding Blocks}
\section{Definitions}

\begin{definition}[Universe]
We consider a random experiment, then the set of all possible outcomes of the experiment is denoted by $\Omega$ and is called the universe.
\end{definition}

\begin{definition}[Event]
An event is usually denoted by $E$. An event is a set of results for which we can compute the probability.
\end{definition}

\begin{definition}[Collection]
The collection of all events is denoted by $\mathcal{F}$. Hence $\mathcal{F} \subseteq \mathcal{P}(\Omega)$.
\end{definition}

\begin{definition}[Disjoint Events]
Two events $A, B \in \mathcal{F}$ are disjoint or incompatible if they cannot occur simultaneously. In other words if $A \cap B = \emptyset$.
\end{definition}

\begin{remark}
We require that the collection $\mathcal{F}$ of the events is an algebra of sets.
\end{remark}

\begin{definition}[Algebra of Sets]
An element $\mathcal{F}$ is called an algebra of sets if $\mathcal{F} \neq \emptyset$ and:
\begin{enumerate}
\item $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$
\item $A, B \in \mathcal{F} \Rightarrow A \cup B \in \mathcal{F}$
\end{enumerate}
\end{definition}

\begin{remark}
For the scope of this course we further require that $\mathcal{F}$ is stable under countable unions. In other words the second condition above is replaced by:
\[
(A_n)_{n \in \mathbb{N}} \in \mathcal{F}^\mathbb{N} \Rightarrow \bigcup_{n \in \mathbb{N}} A_n \in \mathcal{F}
\]
\end{remark}

\begin{definition}[$\sigma$-algebra]
A $\sigma$-algebra is an algebra of sets where the second condition is replaced by the stronger condition requiring stability under countable union.
\end{definition}

\begin{definition}[Probability]
The probability $P(E)$ of $E$ is the theoretical value for the proportion of experiments in which $E$ occurs. Thus the probability is a function from $\mathcal{F}$ to $[0,1]$. Such that:
\begin{enumerate}
\item $P(\Omega) = 1$.
\item $A, B \in \mathcal{F}, A \cap B = \emptyset \Rightarrow P(A \cup B) = P(A) + P(B)$.
\end{enumerate}
In other words, $P$ is an additive set function from $\mathcal{F}$ to $[0,1]$.
\end{definition}

\begin{remark}
This definition however is not very well suited to infinite event sets. Then modern probability theory adds a condition to the above.
\end{remark}

\begin{definition}[Modern Probability]
A modern probability $P(E)$ of $E$ is a probability with the stronger condition:
\[
\forall (A_n)_{n \in \mathbb{N}} \in \mathcal{F}^\mathbb{N}, \left(\forall n, m \in \mathbb{N}, n \neq m \Rightarrow A_n \cap A_m = \emptyset\right) \Rightarrow P\left(\bigcup_{n \in \mathbb{N}} A_n \right) = \sum_{n \in \mathbb{N}} P(A_n)
\]
\end{definition}

\begin{definition}[Probability Space]
A probability space is a triple $(\Omega, \mathcal{F}, P)$. Where $\Omega$ is the universe of all possible results, $\mathcal{F}$ is a $\sigma$-field on $\Omega$, and $P$ is a modern probability function on $\mathcal{F}$.
\end{definition}

\begin{remark}
The mathematical framework which defines probability theory actually comes from another mathematical framework called measure theory. This is why the elements of the $\sigma$-field are sometimes called the measurable sets and the probability function is sometimes called a probability measure.
\end{remark}

\begin{definition}[Finite Space]
We consider the case where $\Omega$ is a finite set, we write $\Omega = \{x_1, \ldots, x_n\}$. The natural $\sigma$-field on $\Omega$ is $\mathcal{P}(\Omega)$. It is the only $\sigma$-field which contains the singletons. Then let $P$ be a probability on $\Omega$ and let us set $\forall i \in \llbracket 1, n\rrbracket, p_i = P(\{x_i\})$. Then the numbers $p_i$ satisfy:
\[
\left( \forall i \in \llbracket 1, n\rrbracket, 0 \leq p_i \leq 1\right) \land \sum_{i = 1}^n p_i = 1
\]
Then for any $A \subset \Omega$ we have by additivity that:
\[
P(A) = \sum_{x \in A} P(\{x\}) = \sum_{i : x_i \in A} p_i
\]
Hence $P$ is completely determined by the numbers $p_i$.
\end{definition}

\begin{remark}
Notice that conversely if we are given the numbers $p_i$ summing to 1 we can define a probability $P$ on $\Omega$ by stating $P(\{x_i\}) = p_i$ and $P$ will indeed be a probability measure.
\end{remark}

\begin{definition}[Countable Spaces]
We suppose that $\Omega$ is countable and we set $\Omega = \{ x_n , n \in \mathbb{N}\}$. The natural $\sigma$-field on $\Omega$ is again the power set of $\Omega$. Then the definitions are an immediate generalization of the ones for a finite space.
\end{definition}

\begin{definition}[Continuous Spaces]
If we take the simplest example of $\Omega = \mathbb{R}$ then the intuitive $\sigma$-field being the power set turns out to be too complicated to be useful. Hence we take for $\mathcal{F}$ the Borel tribe of $\mathbb{R}$, $\mathcal{B}(\mathbb{R})$. The Borel $\sigma$-field corresponds to taking a countable union of all possible closed intervals of $\mathbb{R}$.
\end{definition}

\begin{definition}[Random Variable]
Let $(\Omega, \mathcal{F}, P)$ be a probability space. A random variable $X$ on $(\Omega, \mathcal{F}, P)$ is map from $\Omega$ to $\mathbb{R}$. Which satisfies:
\[
\forall B \in \mathcal{B}(\mathbb{R}), X^{-1}(B) = \{ \omega \in \Omega : X(\omega) \in B \} \in \mathcal{F}
\]
\end{definition}

\begin{remark}
This definition is equivalent to: for all interval $I$ of $\mathbb{R}$ we have that $X^{-1}(I) \in \mathcal{F}$.
\end{remark}

\begin{notation}
The event $X^{-1}(I)$ is denoted by $\{X \in I\}$ or even simply $X \in I$. Secondly random variables are denoted by capital letters typically $X, Y, U, V$ and their possible values are denoted by the corresponding lowercase letters.
\end{notation}

\begin{definition}[Law of a random variable]
Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $X$ be a random variable defined on $\Omega \to \mathbb{R}$. The law of $X$ is the probability measure on $\mathbb{R}$ defined by:
\[
\forall B \in \mathcal{B}(\mathbb{R}), P_X(B) = P(X \in B)
\]
\end{definition}

\begin{proof}
Let us check that $P_X$ is indeed a probability measure. We have that:
\[
P_X(\mathbb{R}) = P(X \in \mathbb{R}) = 1.
\]
Furthermore let $(B_n)_{n \in \mathbb{N}} \in \mathcal{B}(\mathbb{R})^\mathbb{N}$ be a disjoint sequence of Borel sets. Then:
\[
P_X\left(\bigcup_{n \in \mathbb{N}} B_n\right) = P\left(X \in \bigcup_{n\in\mathbb{N}} B_n\right) = P\left( \bigcup_{n\in\mathbb{N}} \{ X \in B_n \} \right) = \sum_{n \in \mathbb{N}} P(X \in B_n) = \sum_{n \in \mathbb{N}} P_X(B_n)
\]
\end{proof}

\begin{notation}
The law $P_X$ of $X$ is sometimes called the \textit{distribution} of $X$. We furthermore say that two variables $X, Y$ have the same law if $P_X = P_Y$. The object of primary interest for a random variable is its law.
\end{notation}

\begin{definition}[Law]
Let $f$ be a non-negative function $\mathbb{R} \to \mathbb{R}^+$ which is integrable and $\int_\mathbb{R} f(x) \dd x = 1$. We define next:
\[
\forall A \in \mathcal{B}(\mathbb{R}) \quad P(A) = \int_A f(x) \dd x
\]
This formula defines a probability measure on $\mathbb{R}$, called the probability measure with density function $f$.
\end{definition}

\begin{definition}[Expectation]
We say that the random variable $X$ has an expectation or that it is integrable if:
\[
\int_\mathbb{R} |x| \dd P_X(x) < +\infty
\]
Then the expectation is defined as:
\[
E(X) = \int_\mathbb{R} x \dd P_X(x) = \int_\Omega X \dd P = \int_{\omega \in \Omega} X(\omega)\dd P(\omega) = \int_\mathbb{R} \text{Id}_\mathbb{R} \dd P_X
\]
From this formula we see that the expectation is completely dependent on the law of the random variable.
\end{definition}

\chapter{Coin tossing games.}

\section{The model}
We take a fair coin and consider the experiment which consists in throwing $n$ times the coin.If -1 denotes tail and +1 denotes head then the result of the experiment is $r \in \{-1, 1\}^n = \Omega$. Since $\Omega$ is finite we immediately have $\mathcal{F} = \mathcal{P}(\Omega)$. Since we assumed the coin to be fair we have by symmetry that all the results are equiprobable. Hence:
\[
\forall \omega \in \Omega, \quad P(\omega) = \frac{1}{|\Omega|} = \frac{1}{2^n}
\]
Now let $X_k$ be the random variable corresponding to the result of the $k$-th throw. Formally $X_k$ is the map:
\begin{align*}
X_k : \Omega &\longrightarrow  X_k(\Omega)\\
(\omega_1, \cdots, \omega_n) &\longmapsto \omega_k 
\end{align*}

\section{Graphical Representation.}
To the sequence $X_1, \cdots, X_n$ we associate the partial sums $S_0 = 0, S_1 = X_1, \cdots, S_n = X_1 + \cdots + X_n$. The sequence $S_0, \cdots, S_n$ contains exactly the same information as $X_1, \cdots, X_n$. We can therefore represent the result of the experiment by a polygonal line, which is the line which joins successively the points: $(0, S_0), \cdots, (n, S_n)$. Such a polygonal line, associated to a sequence of signs, will be called a path.

\section{Interpretation of the model.}
\subsection{Coin tossing game} Potter and Voldemort play the following game. Potter throws a coin and Voldemort tries to guess the result. If Voldemort is wrong he pays 1 euro to Potter however if he is right Potter gives 1 euro to Voldemort. Then $S_n$ represents the algebraic gain of Potter after $n$ turns. 
\subsection{Random Walk} We consider a drunkard which performs a random walk on $\mathbb{Z}$ with the following mechanism. At $t=0$ we center the origin on his position, then at every step he tosses a fair coin and he goes left or right according to whether he obtains tails or heads. Then $S_n$ corresponds to the final position after $n$ steps.
\begin{figure}
\centering
\begin{tikzpicture}
\draw (-2.5, 0) -- (2.5, 0);
\draw (0, 0) node[below] {0};
\draw (0, -0.1) -- (0, 0.1);
\draw (1, -0.1) -- (1, 0.1);
\draw (2, -0.1) -- (2, 0.1);
\draw (-1, -0.1) -- (-1, 0.1);
\draw (-2, -0.1) -- (-2, 0.1);
\draw[->] (0,0) arc(0:180:0.5);
\draw[<-] (1,0) arc(0:180:0.5);
\draw (0.5, 0.5) node[above right] {heads};
\draw (-0.5, 0.5) node[above left] {tails};
\end{tikzpicture}
\end{figure}

\section{Distribution or law of $S_n$}
\begin{proposition}
The law of $S_n$ is the probability distribution on $\{-n, \cdots, n\}$ given by:
\[
\forall k \in \{-n, \cdots, n\} \quad P(S_n = k) = \binom{n}{\frac{n + k}{2}} \frac{1}{2^n}
\]
\end{proposition}
\begin{proof}
A simple proof can be done geometrically. We know that all results are equiprobable hence it suffices to count the number of possible choice that leads to $S_n = k$. Let us consider such a path and let us denote by $\alpha$ the number of ascending steps and $\beta$ the number of descending steps, then we have that:
\[
\begin{cases}
\alpha - \beta = k\\
\alpha + \beta = n
\end{cases}
\Rightarrow \alpha = \frac{n + k}{2}
\] 
Then the number of possible paths is simply given by the number of possible choices for the ascending steps which are immediately given by: $\binom{n}{\alpha}$.
\end{proof}

\section{Equalization or return to 0} We say that there is an equalization or return to 0 at time $n$ if $S_n = 0$. Since $n$ and $S_n$ have the same parity this occurs only at even times. Then from what we got previously we immediately get that:
\[
P(S_{2n} = 0) = \frac{1}{2^{2n}} \binom{2n}{n} = \frac{1}{2^{2n}} \frac{(2n)!}{(n!)^2}
\]
Then Stirling's formula tells us that: $n! = \left(\frac{n}{\ell}\right)^n \sqrt{2\pi n} \left( 1 + \frac{1}{12 n} + \frac{1}{288 n^2} + o(\frac{1}{n^2})\right)$ and applying it above we obtain:
\[
P(S_{2n} = 0) \stackrel{n \to +\infty}{\sim} = \frac{1}{\sqrt{\pi n}}
\]
This gives an excellent approximation even for small values of $n$.

\section{The lamplighter walk.}
Imagine you have an infinite street with lanterns every meter and one lamplighter whose job is to light all the lanterns. He also starts at the origin and lights the lantern at the origin. Then he throws a coin and goes left or right according to if it is tails or heads and lights the successive lantern. The position at time $n$ is given by $S_n$ and the process $(S_n)_{n \in \mathbb{N}}$ is the symmetric random walk on $\mathbb{Z}$. Now we are interested in the following question: What is the probability that the lamplighter comes back to the original lantern? Mathematically this is described by:
\begin{align*}
P(\exists n \in \mathbb{N}^*, S_{2n} = 0) &= P\left(\bigcup_{n \geq 1} \{ S_{2n} = 0 \} \right) \\
&= P\left(\{S_2 = 0\} \cup (\{S_4 = 0\}\setminus\{S_2 = 0\}) \cup \cdots \cup (S_{2n = 0} \setminus (\{S_2 = 0\} \cup \cdots \cup \{S_{2n - 2} = 0\} )\right)\\
&= P\left(\bigcup_{n \geq 1} \left( \left\{S_{2n} = 0\right\} \setminus \left( \left\{S_2 = 0\right\} \cup \cdots \cup \left\{ S_{2n - 2} \right\} \right) \right)\right)
\end{align*}
Now since the events are disjoint we can use the $\sigma$-additivity property. Hence we get:
\[
P(\exists n \in \mathbb{N}^*, S_{2n} = 0) = \sum_{n \geq 1} P(S_2 \neq 0, \cdots, S_{2n - 2} \neq 0, S_{2n} = 0)
\]
Now necessarily all the $S_{2i}$ for $i \in \llbracket 1, n-1\rrbracket$ must have the same sign. By symmetry the probability that they are all positive is equal to the probability that they are all negative. Hence:
\begin{align*}
P(\exists n \in \mathbb{N}^*, S_{2n} = 0) &= \sum_{n\geq 1} 2 P(S_1 > 0, \cdots, S_{2n-1} > 0, S_{2n} = 0) \\
&= \sum_{n \geq 1} \frac{2}{2^{2n}} |\{ \text{paths from } (1, 1) \text{ to } (2n -1, 1) \text{ which do not touch the axis} \}|
\end{align*}
Now we decide to count the number of paths that do touch the axis instead of the one that do not touch the axis. Since we know the total number of paths this is an equivalent problem. 

\section{Reflection principle.} Let $A = (a, \alpha)$ and $B = (b, \beta)$ be two points with $0 < a < b$ and $\alpha, \beta > 0$. My problem now is to count the number of paths that go from $A$ to $B$ which touch or cross the axis. Notice that this problem is perfectly equivalent to counting the number of paths that go from $A' = (a, -\alpha)$ to $B$. To formalize this we need to make sure that this is a one-to-one map. Consider a path $s = (s_a, s_{a+1}, \cdots, s_b)$ be a path from $A$ to $B$ which touches the axis and $t$ be the first time it touches in other words $t = \min\{ i \geq a : s_i = 0\}$. Let $T = (t, 0)$ and to the path $s$ we associate the path $\phi(s)$ obtained from $s$ by taking the reflexion of the portion $AT$ with respect to the $x$-axis. Now we claim that $\phi$ is a one-to-one correspondence from the paths from $A$ to $B$ which touch the axis unto the paths from $A'$ to $B$. In fact $\phi$ is an involution ($\phi^2 = \text{Id}$) and hence it is injective and obviously surjective therefore bijective. 

\section{The ballot theorem.} Let $x, n > 0$. The number of paths from $(0,0)$ to $(n, x)$ which do not touch the axis after time $0$ is equal to the number of paths from $(1, 1)$ to $(n,x)$ minus the number of paths from $(1, -1)$ to $(n, x)$. This immediately yields:
\[
\frac{x}{n}\binom{n}{\frac{n+x}{2}}
\]  
An example of application is the following. In an election the candidate $P$ scores $p$ votes and $Q$ scores $q$ votes with $p > q$. The probability that the winning candidate is always ahead during the reading of the votes is given by:
\[
\frac{p - q}{p + q}
\]

\section{End of the computation.}
Now going back to our lamplighter computation applying the reflexion principle we have that:
\[
P(\exists n \in \mathbb{N}^*, S_{2n} = 0) = \sum_{n \geq 1} \frac{2}{2^{2n}} \frac{1}{2n - 1} \binom{2n - 1}{n} = \sum_{n \geq 1} \frac{1}{2n - 1} P(S_{2n} = 0)
\]
Hence the probability that the lamplighter returns to 0 is given by:
\[
P(\text{return to 0}) = \sum_{n \geq 1} \frac{1}{2n -1} P(S_{2n} = 0) = \sum_{n \geq 1} P(S_{2n - 2} = 0 ) - P(S_{2n} = 0) = 1
\]

\section{Fundamental Lemma}
From the previous computation we have gotten that:
\[
P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0, S_{2n} = 0) = P(S_{2n - 2} = 0) - P(S_{2n} = 0)
\]
Yet we also have that:
\[
P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0, S_{2n} = 0) = P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0) - P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0, S_{2n} \neq 0)
\]
Hence for any $n$ larger than 1 we have that:
\[
P(S_{2n - 2} = 0) - P(S_{2n} = 0) = P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0) - P(S_2 \neq 0, \cdots, S_{2n - 2}\neq 0, S_{2n} \neq 0)
\]
Moreover we have that:
\[
P(S_2 \neq 0 ) = \frac{1}{2} = P(S_2 = 0)
\]
Which gives the fundamental lemma:
\[
P(S_2 \neq 0, \cdots, S_{2n} \neq 0) = P(S_{2n} = 0)
\]

\section{Last tie}
Consider a coin tossing game of length $2n$ and the time $T$ of the last tie before $2n$: $T = \max\{k \leq 2n : S_k = 0\}$. Now we want to find the distribution of $T$. 
\begin{proposition}
The law of $T$ is called the arcsinus law and is given by:
\[
P(T = k) = P(S_{2k} = 0)P(S_{2n - 2k} = 0)
\]
\end{proposition}

\chapter{Independence} 

\section{Conditional Probability.}

Let $(\Omega, \mathcal{F}, P)$ be a probability space and $A, B \subset \mathcal{F}$ with $P(B) > 0$. Then we have that the conditional probability of A given B is as follows:
\[
P(A | B) = \frac{P(A \cap B)}{P(B)}
\]
This formula however does not hold in the general case where $P(B) = 0$ where the treatment is much more delicate. Now notice that $B$ being fixed the application $A \in \mathcal{F} \mapsto P(A|B)$ is a probability measure on $\mathcal{F}$. 

\section{Sub $\sigma$-field and generated $\sigma$-fields}
A sub $\sigma$-field  $\mathcal{G}$ of $\mathcal{F}$ is a $\sigma$-field $\mathcal{G}$ such that $\mathcal{G} \subset \mathcal{F}$. Now let $\mathcal{A}$ be a collection of parts of $\Omega$. The $\sigma$-field generated by $\mathcal{A}$ denoted by $\sigma(\mathcal{A})$ is the smallest $\sigma$-field on $\Omega$ which contains all the elements of $\mathcal{A}$. Formally it is defined as being the intersection of all the $\sigma$-fields which contain $\mathcal{A}$. Now let $E$ be a set equipped with a $\sigma$-field $\mathcal{E}$. Now we start by generalizing the concept of random variable. A random variable $X$ taking values in $E$ is a map $X : \Omega \to E$ such that:
\[
\forall A \in \mathcal{E}, \quad X^{-1}(A) = \left\{ \omega \in \Omega : X(\omega) \in A \right\} \in \mathcal{F}
\] 
Then formally the $\sigma$-field generated by the random variable $X$ is given by:
\[
\sigma(X) = \{ X^{-1}(A) : A \in \mathcal{E}\}
\]
We leave it as an exercise to the reader to check that this does indeed define a $\sigma$-field.

\section{Fundamental definition of independence}

\begin{definition}[Two independent events]
Let $(\Sigma, \mathcal{F}, P)$ be a probability space. Then 2 events $A, B \in \mathcal{F}$ are said to be independent if and only if $P(A \cap B) = P(A) P(B)$. 
\end{definition}

\begin{remark}
Notice right away that if $A, B$ are independent then $A, B^c$ are independent and so is $A^c, B^c$. Furthermore if $P(B) > 0$ then using the definition of conditional probability we can write it as $P(A | B) = P(A)$. This is a representation of the intuitive notion: "if $A$ does not depend on $B$ then the advent of $B$ should change nothing to result of $A$".  
\end{remark}

\begin{definition} [Multiple independent events] \label{indp:events} Consider $n$ events $A_1, \cdots, A_n$. They are said to be independent if and only for all finite subset $I$ of $\{1, \cdots, n\}$ we have:
\[
P\left( \bigcup_{i \in I} A_i \right) = \prod_{i \in I} P(A_i)
\]
\end{definition}

\begin{remark}
Notice that this statement is much stronger than the one requiring for the events to be independent two-by-two. Don't fall in this common trap!
\end{remark}

\begin{definition}[Multiple independent real random variables] \label{indp:rv} Consider $n$ random variables $X_1, \cdots, X_n : \Omega \to \mathbb{R}$. They are said to be independent if $\forall B_1, \cdots, B_n \in \mathcal{B}(\mathbb{R})$ we have:
\[
P(X_1 \in B_1, \cdots, X_n \in B_n) = \prod_{i = 1}^n P(X_i \in B_i)
\]
\end{definition}

\begin{definition}[Multiple independent sub $\sigma$-fields] \label{indp:sigma} Consider $n$ $\sigma$-fields $\mathcal{F}_1, \cdots, \mathcal{F}_n$ of $\mathcal{F}$. They are said to be independent if $\forall A_1 \in \mathcal{F}_1, \cdots, A_n \in \mathcal{F}_n$ we have:
\[
P(A_1 \cap \cdots \cap A_n) = P(A_1) \cdots P(A_n)
\]
\end{definition}

\begin{remark}
Notice however that some of the above definitions are stronger than others.
\end{remark}

\begin{proposition}

The Definition \ref{indp:events} can be reformulated from the Definition \ref{indp:rv} by taking for random variables the indicator functions of each event. Similarly the Definition \ref{indp:rv} can be reformulated from the Definition \ref{indp:sigma} by taking for $\sigma$-fields the fields generated by the random variables: $\sigma(X_1), \cdots, \sigma(X_n)$. Since the formula for $\sigma$-fields is the most general it is the one that is usually used as the definition for independence. 

\end{proposition}

\begin{remark}

This allows us to treat case like when we consider $n$ spaces given by $(E_1, \mathcal{E}_1), \cdots, (E_n, \mathcal{E}_n)$ and $n$ random variables on these spaces. Then the $\sigma$-field definition allows us to consider whether the r.v. are independent from each other or not even if a certain $X_i$ might be a function and another one a matrix or a permutation.  

\end{remark}


\section{Product Law}

From now on let $(E_1, \mathcal{E}_1, \mu_1), \cdots, (E_n, \mathcal{E}_n, \mu_n)$ be $n$ probability spaces. The question we will try to answer is: Does there exist a probability space $(\Sigma, \mathcal{F}, P)$ on which are defined $n$ r.v. $X_1, \cdots, X_n$ such that $X_1, \cdots, X_n$ are independent and $X_i$ takes values in $E_i$ and has for law $P_{X_i} = \mu_i$?

\begin{remark} We remind here the definition of the law of $X_i$. The law $P_{X_i}$ of $X_i$ is the probability measure on $E_i$ defined by $\forall A \in \mathcal{E}_i, \quad P_{X_i}(A) = P(X_i \in A)$. 
\end{remark}

\begin{definition}[Product probability space] Consider $\Omega = E_1 \times \cdots \times E_n$ with the $\sigma$-field $\mathcal{F} = \mathcal{E}_1 \otimes \cdots \otimes \mathcal{E}_n$. In other words $\mathcal{F}$ is the $\sigma$-field generated by the sets of the form: $A_1 \times \cdots \times A_n$ where $A_i \in \mathcal{E}_i$. 
\end{definition}

\begin{theorem} [Product Measure] There exists a unique probability measure on $\mathcal{F}$ such that: 
\[
\forall A_1 \in \mathcal{E}_1, \cdots, A_n \in \mathcal{E}_n \quad \mu(A_1 \times \cdots \times A_n) = \mu_1(A_1) \cdots \mu_n(A_n)
\]
This measure is called the measure product and is denoted by $\mu = \mu_1 \otimes \cdots \otimes \mu_n$. 
\end{theorem}

\begin{definition}[Product Law]
We then take $(\Omega, \mathcal{F}, \mu)$ as defined above and define $n$ r.v. on $X_1, \cdots, X_n$ on $(\Omega, \mathcal{F}, \mu)$ by taking:
\begin{align*}
X_i : \Omega &\longrightarrow E_i\\
\omega = (\omega_1, \cdots, \omega_n) &\longmapsto X_i(\omega) = \omega_i
\end{align*}
Which is simply the projection on the $i$-th coordinate. Then $X_i$ takes values in $E_i$ and has for law $P_{X_i} = \mu_i$. Indeed for any $A_i \in \mathcal{E}_i$ we have:
\[
P_{X_i}(A_i) = \mu(X_i \in A_i) = \mu (X_i^{-1}(A_i)) = \mu(E_1 \times \cdots \times E_{i - 1} \times A_i \times E_{i + 1} \times \cdots \times E_n) = \mu_i(A_i)
\]
We now claim that $X_1, \cdots, X_n$ are independent. To check this take $A_1 \in \mathcal{E}_1, \cdots, A_n \in \mathcal{E}_n$ then we have that:
\begin{align*}
\mu(X_1 \in A_1, \cdots, X_n \in A_n) &= \mu(X_{1}^{-1}(A_1) \cap \cdots \cap X_n^{-1}(A_n)) = \mu(A_1 \times \cdots \times A_n) = \mu_1(A_1) \cdots \mu_n(A_n) \\
&= \mu(X_1 \in A_1) \cdots \mu(X_n \in A_n)
\end{align*}
\end{definition}

\begin{remark}
A simple application is given by $E_1 = \cdots = E_n = \mathbb{R}$ and $\mathcal{E}_1 = \cdots = \mathcal{E}_n = \mathcal{B}(\mathbb{R})$. Then being given $n$ laws $\mu_1, \cdots, \mu_n$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. There exists a space $(\Omega, \mathcal{F}, P)$ on which there are defined $n$ i.r.v. $X_1, \cdots, X_n$ of laws $\mu_1, \cdots, \mu_n$. When $\mu_1 = \cdots = \mu_n$ we say the the random variables $X_1, \cdots, X_n$ are independent and identically distributed. 
\end{remark}

\begin{remark}
We are going to take the binomial as an example. Let $X$ which follows a Bernouilli law of parameter $p$ which we denote $X \sim \text{Bernouilli}(p)$ which corresponds to $P(X = 0) = 1- p, P(X = 1) = p$. Then let $X_1, \cdots, X_n$ be $n$ independent and identically distributed (i.i.d.) random variables following a Bernouilli law of parameter $p$. Then we write $S_n = X_1 + \cdots + X_n$. Then $S_n$ follows the binomial law of parameters $n$ and $p$ which is usually denoted by $S_n \sim B(n, p)$. 
\end{remark}

\section{Block regrouping}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $\mathcal{F}_1, \cdots, \mathcal{F}_n$ be $n$ independent sub $sigma$-fields of $\mathcal{F}$.

\begin{lemma} We prove a small lemma that will be needed for the proof of the following theorem. Let $P_1,P_2$ be 2 probability measures on $\mathcal{F}$ such that there exists $\mathcal{A}$ a part of $\mathcal{F}$ such that:
\begin{enumerate}
\item $\forall A \in \mathcal{A}, P_1(A) = P_2(A)$
\item $\sigma(\mathcal{A}) = \mathcal{F}$
\item $\mathcal{A}$ is stable under finite intersection.
\end{enumerate}
Then $P_1 = P_2$ on $\mathcal{F}$. 
\end{lemma}

\begin{theorem}
Let $I, J$ be two disjoint subsets of $\{1, \cdots, n\}$. Then the $\sigma$-fields $\mathcal{I} = \sigma(\mathcal{F}_i, i \in I), \mathcal{J} = \sigma(\mathcal{F}_j, j \in J)$ are independent.
\end{theorem}

\begin{proof} What we want to show is:
\[
\forall D \in \mathcal{I}, \forall E \in \mathcal{J} \quad P(D \cap E) = P(D) P(E)
\]
We start by supposing $E = \bigcap_{j\in J} E_j$ where $E_j \in \mathcal{F}_j$, now if $D$ is of the same form $D = \bigcap_{i \in I} D_i$ where $D_i \in \mathcal{F}_i$, then the result follows immediately from the independence of $\mathcal{F}_1, \cdots, \mathcal{F}_n$. Now let's fix $E$ as before and suppose that $P(E) > 0$. Now we write:
\[
P_1(D) = P(D | E) \mbox{~~and~~} P_2(D) = P(D)
\] 
Then $P_1$ and $P_2$ are two probability measures on $\mathcal{I}$. Furthermore $P_1(D) = P_2(D)$ is $D$ is of the form $D = \bigcap_{i \in I} D_i$ with $D_i \in \mathcal{F}_i$. Now from the lemma the result follows. We can now repeat the same argument reversing the roles of $E$ and $D$ and conclude.
\end{proof}

\begin{corollary}
Let $\mathcal{F}_1, \cdots, \mathcal{F}_n$ be $n$ independent sub $\sigma$ fields. Then $I_1, \cdots, I_n$ be $n$ parts of $\{1, \cdots, n\}$ two-by-two disjoint. Then the $\sigma$ fields $\mathcal{I}_i = \sigma(\mathcal{F}_j, j \in I_i)$ are independent. The proof follows from induction using the previous proof.
\end{corollary}

\begin{corollary}

Let $X_1, \cdots, X_n$ be $n$ random variables and $I_1, \cdots, I_n$ be $n$ disjoint parts of $\{1, \cdots, n\}$. Then the $\sigma$-fields $\sigma(X_j, j \in I_i)$ are independent. 

\end{corollary}

\section{Expectancy and independence.}

\begin{proposition}
Let $X, Y$ be two random variables on $\Omega$ with values in $\mathbb{R}$ then $X, Y$ are independent if and only if for all borellian functions $f, g$ of $\mathbb{R} \to \mathbb{R}$ such that $E[|f(X)|] < +\infty$ and $E[|g(Y)|] < +\infty$ we have that $E[|f(X) g(Y)|] < +\infty$ and $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$. 
\end{proposition}

\begin{remark} We give here the definition of a borellian function from $\mathbb{R}$ to $\mathbb{R}$. A Borellian function $f$ from $\mathbb{R}$ to $\mathbb{R}$ is a function such that: $\forall B \in \mathcal{B}(\mathbb{R}), f^{-1}(B) \in \mathcal{B}(\mathbb{R})$. 
\end{remark}

\begin{proof}
We give here only an idea of the proof. The right to left implication is easy it suffices to take $f = 1_A$ and $g = 1_B$ and then the result follows immediately. Now the other way around if $X, Y$ are independent than for any $A, B \in \mathcal{B}(\mathbb{R})$ we have that $P(X \in A, Y \in B) = P(X \in A) P(Y \in B)$. Then we with the same reasoning we know that the second proposition is true for $f=  1_A$ and $g = 1_B$ the trick now is to, as we did previously, extend this result to any Borellian functions. Linearity automatically extends the allowed functions to a linear combination of identities. Then for any Borellian functions $f$ and $g$ we can approximate them as a limit of functions of this form.  
\end{proof}

\begin{corollary}
Let $X, Y$ be two independent random variables on $(\Omega, \mathcal{F}, P)$ with finite expectancy $E[|X|] < +\infty, E[|Y|] < +\infty$. Then $E[|XY|] < + \infty$ and $E[XY] = E[X]E[Y]$. 
\end{corollary}

\begin{proof}
This follows directly from the proposition by taking $f = g = \text{Id}$. 
\end{proof}

\begin{remark}
Notice however that one can come back from the corollary to the proposition by simply taking $X' = f(X)$ and $Y' = f(Y)$. 
\end{remark}

\begin{remark} We give an application of such a proposition. Let $X$ be a random variable of finite expectancy and such that $E[X^2] < +\infty$. Then given another random variable $Y$ we define the following:
\[
cov(X, Y) = E[( X - E[X]) (Y - E[Y])]
\]
Then if $X$ and $Y$ are independent and their squares have finite expectancy then $V(X + Y) = V(X) + V(Y)$ and $cov(X, Y) = 0$. 
\end{remark}

\section{Independence and law}
\begin{theorem}
Let $X_1, \cdots, X_n$ be $n$ random variables defined on $(\Omega, \mathcal{F}, P)$. Then the following statements are equivalent:
\begin{enumerate}
\item $X_1, \cdots, X_n$ are independent.
\item $\forall f_1, \cdots, f_n : \mathbb{R} \to \mathbb{R}$ borellian such that $E[|f_{i}(X_i)|] < +\infty$ we have that $E[|f_1(X_1) \cdots f_n(X_n)|] < +\infty$ and $E[f_1(X_1) \cdots f_n(X_n)] = E[f_1(X_1)] \cdots E[f_n(X_n)]$. 
\item $P_{X_1, \cdots, X_n} = P_{X_1} \otimes \cdots \otimes P_{X_n}$.
\end{enumerate}
\end{theorem}

\begin{remark}
The law $P_{X_1, \cdots, X_n}$ is the joint law on $(X_1, \cdots, X_n)$ and it is a probability measure on $\mathbb{R}^n$ defined by $P_{X_1, \cdots, X_n}(A) = P((X_1, \cdots, X_n) \in A)$ where $A$ is a borelian of $\mathbb{R}^n$. Then the $P_{X_1} \otimes \cdots \otimes P_{X_n}$ is the product law as defined previously.  
\end{remark}

\chapter{Infinite sequences of random variables.}

\section{A random uniform number in $[0, 1]$}
We would like to build a mathematical model for the experiment which consists in drawing a random number in $[0,1]$ with uniform distribution. We take $\Omega = [0, 1]$ and $\mathcal{F} = \mathcal{B}([0, 1])$ where $\mathcal{B}([0, 1])$ is the Borel $\sigma$-field of $[0, 1]$ which can be defined as the trace or the restriction of $\mathcal{B}(\mathbb{R})$ to $[0, 1]$, i.e. :
\[
\mathcal{B}([0, 1]) = \{ B \cap [0, 1] : B \in \mathcal{B}(\mathbb{R})\} = \{ B \in \mathcal{B}(\mathbb{R}), B \subset [0, 1]\}
\]

\begin{theorem} [Lebesgue Measure]
There exists a unique probability measure $\lambda$ on $\mathcal{B}([0,1])$ such that: $\forall a < b \in [0,1],\, \lambda([a,b]) = b- a$. This probability measure $\lambda$ is called the Lebesgue measure. 
\end{theorem}

\begin{proof}
We give here a sketch of the proof. We define for any subset $A$ of $[0, 1]$ the following:
\[
\lambda^*(A) = \inf \left\{ \sum_{i \in I} b_i - a_i : a_i < b_i, A \subset \bigcup_{i \in I} ]a_i, b_i[\right\}
\] 
We would like for $\lambda^*$ to be a probability measure however it is not the case since it is not $\sigma$-additive but $\sigma$-sub-additive. However $\lambda^*$ restricted to $\mathcal{B}([0, 1])$ is a genuine probability measure. However to prove this is a hard work. Then we define $\lambda$ as the restriction of $\lambda^*$ to $\mathcal{B}([0,1])$. We have the following facts:
\[
\forall x_0 \in [0, 1], \lambda(\{x_0\}) = 0 \mbox{~~and~~} \forall x_i \in [0,1], \lambda\left( \bigcup_{i \in \mathbb{N}} x_i \right) = 0
\]
Furthermore there exist also subsets of $[0,1]$ which are not countable and whose $\lambda$-measure is $0$, such as the Cantor set for example.
\end{proof}

\section{Convergences}
Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $(X_n)_{n \in \mathbb{N}}$ be a sequence of random variables defined on $\Omega$. Let $X$ be another random variable.

\begin{definition}[Converging almost surely]
The sequence $(X_n)_{n \in \mathbb{N}}$ is said to converge almost surely towards $X$ if and only if $P(\{\omega : \lim_{n\to+\infty} X_n(\omega) = X(\omega)\}) = 1$. This is denoted by $X_n \stackrel{a.s.}{\longrightarrow} X$. 
\end{definition}

\begin{definition}[$L^p$ mean convergence]
The sequence $(X_n)$ converges in $L^p$ mean towards $X$ if and only if $\lim_{n \to +\infty} E(|X_n - X|^p) = 0$. This is denoted by $X_n \stackrel{L^p}{\longrightarrow} X$. 
\end{definition}

\begin{definition}[Converging in probability]
The sequence $(X_n)$ converges in probability towards $X$ if and only if $\forall \varepsilon > 0, \lim_{n \to +\infty} P(|X_n - X| > \varepsilon) = 0$. This is denoted by $X_n \stackrel{P}{\longrightarrow} X$. 
\end{definition}

\section{Links between the different convergences.}
The convergence which is usually the most difficult to establish is the first one. 

\begin{lemma}
We start with a lemma which we will need for the following proof. If $(A_n)$ is a non-increasing sequence of events, i.e., $A_{n+1}\subset A_n$ then $P(\bigcap_n A_n) = \lim_{n \to \infty} P(A_n)$. 
\end{lemma}

\begin{proposition}
If $X_n \stackrel{a.s.}{\longrightarrow} X$ then $X_n \stackrel{P}{\longrightarrow} X$. 
\end{proposition}

\begin{proof}
Suppose that $X_n \stackrel{a.s.}{\longrightarrow} X$. Then:
\begin{align*}
P(\{\omega : \lim_{n \to \infty} X_n(\omega) = X(\omega)\}) = 1 &\Leftrightarrow P(\forall \varepsilon > 0, \exists N, \forall n \geq N, |X_n - X| < \varepsilon) = 1\\
&\Leftrightarrow P\left(\bigcap_{\varepsilon > 0, \varepsilon \in \mathbb{Q}} \bigcup_{N \geq 1} \bigcap_{n \geq N} \{|X_n - X| < \varepsilon\} \right) = 1\\
&\Leftrightarrow \forall \varepsilon > 0, \varepsilon \in \mathbb{Q}, P(\bigcup_{N \geq 1} \bigcap_{n \geq N} \{|X_n - X| < \varepsilon\}) = 1\\
&\Leftrightarrow \forall \varepsilon > 0, \varepsilon \in \mathbb{Q}, P(\bigcap_{N \geq 1} \bigcup_{n \geq N} \{|X_n - X| < \varepsilon\}) = 0\\
&\Leftrightarrow \forall \varepsilon > 0, \lim_{N \to \infty} P(\bigcup_{n \geq N} \{|X_n - X| < \varepsilon\}) = 0 \\
&\Rightarrow \forall \varepsilon > 0, \lim_{N \to \infty} P(|X_n - X| > \varepsilon) = 0 \Leftrightarrow X_n \stackrel{P}{\longrightarrow} X 
\end{align*}
\end{proof}

\begin{proposition}
Let $r \geq 1$. If $X_n \stackrel{L^r}{\longrightarrow} X$ then $X_n \stackrel{P}{\longrightarrow} X$. 
\end{proposition}

\begin{proof}
Suppose that $X_n \stackrel{L^r}{\longrightarrow} X$, i.e. $\lim_{n \to \infty} E(|X_n - X|^r) = 0$. Which we can re-write as:
\[
E(|X_n - X|^r) = \int_\Omega |X_n - X|^r \dd P = \int_{|X_n - X| > \varepsilon} |X_n - X|^r \dd P + \int_{|X_n - X| \leq \varepsilon} |X_n - X|^r \dd P 
\]
Which gives us the following lower-bounds:
\[
E(|X_n - X|^r) \geq \int_{|X_n - X| > \varepsilon} \varepsilon^r \dd P = \varepsilon^r P(|X_n - X| \geq \varepsilon)
\]
Hence we get:
\[
P(|X_n - X| > \varepsilon) \leq \varepsilon^{-r} E(|X_n - X|^r) \stackrel{n \to \infty}{\longrightarrow} 0 
\]
\end{proof}

\begin{proposition}

Suppose $X_n \stackrel{\star}{\longrightarrow}$ and $Y_n \stackrel{\star}{\longrightarrow} Y$ where $\star = a.s.$, or $L^p$ or $P$. Then $X_n + Y_n \stackrel{\star}{\longrightarrow} X+ Y$. 

\end{proposition}

\begin{proof}
The cases where $\star = a.s.$ or $L^r$ are quite classical. Let us look at $\star = P$. To do so we bound the following probability:
\[
P(|X_n + Y_n - (X + Y)| > \varepsilon) \leq P(|X_n - X| > \frac{\varepsilon}{2}) + P(|Y_n - Y| > \frac{\varepsilon}{2})
\]
\end{proof}

\begin{theorem}[Dominated Convergence Theorem]
Let $(X_n)$ be a sequence of random variables $\Omega \to \mathbb{R}$ and suppose that $X_n \stackrel{a.s}{\longrightarrow} X$, and there exists a random variable $Y$ such that $\forall n,\, |X_n| \leq Y$ and $E[Y] < \infty$. Then $X_n \stackrel{L^1}{\longrightarrow} X$. 
\end{theorem}

\section{Examples and counter examples.}
We consider the space $(\Omega, \mathcal{F}, P)$ which is $([0,1], \mathcal{B}([0, 1]), \lambda)$. We consider the sequence $(X_n)$ defined as follows:
\[
X_{2n} = \begin{cases} 1 \mbox{~~if~~} \omega < \frac{1}{2}\\
0 \mbox{~~otherwise}
\end{cases} \mbox{~~and~~} X_{2n+1} = \begin{cases} 0 \mbox{~~if~~} \omega < \frac{1}{2}\\
1 \mbox{~~otherwise}
\end{cases}
\]
Then $X_n$ does not converge according to any definition. Yet the distribution of $X_n$ is $Bernouilli(\frac{1}{2})$. 

\section{Independence for an infinite sequence}

\begin{definition}
A sequence $(\mathcal{F}_n)$ of sub $\sigma$-fields of $\mathcal{F}$ (respectively of r.v. $(X_n) : \Omega \to \mathbb{R}$ or events $(A_n)$) is said to be independent if any finite sub-family extracted from $(\mathcal{F}_n)$ is independent. 
\end{definition}

\begin{theorem}[Block grouping] Let $(\mathcal{F}_n)$ be an independent sequence of sub $\sigma$-fields of $\mathcal{F}$. Let $(I_k)$ be a sequence of subsets of $\mathbb{N}$ which are pairwise disjoint. Then the $\sigma$-fields: $J_k = \sigma(\mathcal{F}_i, i \in I_k)$ are independent. 

\end{theorem}

\begin{proof}
Same argument as the finite case. 
\end{proof}

\section{An analytic model for coin tossing.}
We want to construct a probability space $(\Omega, \mathcal{F}, P)$ such that we can define an infinite sequence $(X_n)$ of Bernouilli i.i.d.r.v. on $\Omega$. We consider the space $(\Omega, \mathcal{F}, P) = ([0, 1], \mathcal{B}([0,1]), \lambda)$. Let $\omega \in [0, 1]$ and let us consider the dyadic or binary expansion of $\omega = 0, \omega_1 \omega_2 \cdots = \sum_{n \geq 1} \frac{\omega_n}{2^n}$. If we exclude sequences which are constantly equal to 1 after a finite rank the expansion is unique and we define $X_n : \Omega \to \{-1, +1\}$ by $X_n(\omega) = 2\omega_n - 1$. Thus $X_n$ is a Bernouilli random variable. Let $(\varepsilon_1, \cdots, \varepsilon_n) \in \{-1, 1\}$ then:
\[
P(X_i = \varepsilon_i) =  \lambda(\omega : \frac{1 + \varepsilon_1}{2} + \cdots + \frac{1 + \varepsilon_n}{2^n} \leq \omega < \frac{1 + \varepsilon_1}{2} + \cdots + \frac{1 + \varepsilon_n}{2^n} + \frac{1}{2^n}) = \frac{1}{2^n}
\]  
Since this is true for any $n$ we also have that the $(X_n)$ are independent. We can now use this model for the coin tossing game and formulate questions dealing with an infinite number of coins. For instance the return to 0. 

\section{An infinite sequence of i.i.d.r.v.}
Often one wants to defined an infinite sequence of r.v. with any given law however the existence of such a structure is a difficult question. 


\chapter{Transience and recurrence of random walks in $\mathbb{Z}^d$.}
Throughout this chapter we consider a sequence $(X_n)_{n \in \mathbb{N}}$ of i.i.d. random variables with values in $\mathbb{Z}^d$ where $d \geq 1$. We study the associated random walk defined by $S_0 = \vb{0}$ and for any $n\geq 1$ we have $S_n = \sum_{i = 1}^n X_i$. 

\section{Return to $\vb{0}$ and recurrence.}

\begin{proposition}
If the random walk returns to $\vb{0}$ with probability one then it will return to $\vb{0}$ infinitely many times with probability one. 
\end{proposition}

\begin{proof}
We suppose that $P(\exists n \in \mathbb{N}^*, S_n = 0) = 1$. Let us introduce the time $R$ of the last visit to $\vb{0}$. Let $k \in \mathbb{N}$ and let us examine:
\[
P(R = k) = P(S_k = 0, \forall \ell > k, S_\ell \neq 0) = P(S_k = 0, \forall \ell > k, S_\ell - S_k \neq 0)
\]
Now we have that $S_k$ is independent from the sequence $S_\ell - S_k$ and hence from the block grouping theorem we can factorize this as:
\[
P(S_k = 0)P(\forall \ell > k, S_\ell - S_k \neq 0) = P(S_k = 0)P(\forall n \geq 1, S_n \neq 0) \leq 1 \cdot (1 - P(\exists n \in \mathbb{N}^*, S_n = 0)) = 0
\]
Now the probability that $S_n$ returns to $\vb{0}$ a finite number of times is given by the probability that $R < +\infty$ which from the above is 0.

\end{proof}

\begin{corollary}

Equivalence:

\begin{enumerate}

\item $P(\exists n\in\mathbb{N}^*, S_n=0)=1$

\item $P(S_n=0\text{ infinitely often})=1$

\end{enumerate}

\end{corollary}


Often, it is easier to prove that something happens infinitely many times with probability one than to prove that it happens once with probability one!


\begin{definition}

The random walk $(S_n)_{n\in\mathbb{N}}$ is said to be recurrent if it returns to 0 infinitely many times with probability one, and transient otherwise. 

\end{definition}


\section{Borel-Cantelli Lemma}

Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $(A_n)_{n\in\mathbb{N}}$ be a sequence of events of $\mathcal{F}$. We define the event 

$$\{A_n\text{ occurs infinitely often }\}=\{A_n:\,\,i.o.\}=\limsup_nA_n=\bigcap_{n\in\mathbb{N}}(\bigcup_{m\geq n}A_m)$$

\begin{lemma}(Borel-Cantelli part 1) If the series $\sum_{n\geq 0}P(A_n)$ converges, then 

$$P(A_n:\,\,i.o.)=0$$

\end{lemma}



\begin{lemma}(Borel-Cantelli part 2)If the series $\sum_{n\geq 0}P(A_n)$ diverges and if the events $(A_n)$ are independent, then 

$$P(A_n:\,\,i.o)=1$$

\end{lemma}

\proof (part 1). We suppose that $\sum_{n\geq 0}P(A_n)<\infty$. We have that $P(A_n:\,\,i.o.)=P(\bigcap_{n\geq 0}(\bigcup_{m\geq n}A_m))=\lim_{n\to\infty}P(\bigcup_{m\geq n} A_m)\leq_{\sigma-subadditivity}\lim_{n\to\infty}\sum_{m\geq n}P(A_m)=0$


\proof (part 2). We suppose that $\sum_{n\geq 0}P(A_n)=\infty$ and $(A_n)$ are independent, then

$$P(\{A_n\,\,i.o.\}^C)=P(A_n\text{ occurs finitely many times})=P((\bigcap_{n\geq 0}\bigcup_{m\geq n} A_m)^C)=P(\bigcap_{n\geq 0}\bigcup_{m\geq n} A_m^C)\leq \sum_{n\geq 0}P(\bigcap_{m\geq n}A_m^C)$$

$$=\lim_{N\to\infty}P(\bigcap_{n\leq m\leq N}A_m^C)=_{indep.}\lim_{N\to\infty}\prod_{n\leq m\leq N}P(A_m^C)=\lim_{N\to\infty}\prod_{n\leq m\leq N}(1-P(A_m))$$

then we take the logarithm:

$$\ln\lim_{N\to\infty}\prod_{n\leq m\leq N}(1-P(A_m))=\sum_{n\leq m\leq N}\ln(1-P(A_m))\leq -\sum_{n\leq m\leq N}P(A_m)\to_{N\to\infty}-\infty$$

therefore $\ln\lim_{N\to\infty}\prod_{n\leq m\leq N}(1-P(A_m))\to-\infty$ hence $P(\bigcap_{m\geq n}A_m^C)$, $\forall n\in\mathbb{N}$.

\begin{remark}
An example of an application is the monkey and the typewriter statement. If you let a monkey type at a typewriter with probability one he will eventually work the entire works of Shakespeare. To simplify we resume the works to a binary alphabet then model the output by $(X_n)$ i.i.d.r.v. Bernouilli r.v. of parameter $p$. Then let $\varepsilon_1, \cdots, \varepsilon_\ell$ be the sequence representing the whole works of Shakespeare. Then:
\[
A_n = \{X_n = \varepsilon_1, \cdots, X_{n+\ell - 1} = \varepsilon_\ell \} \mbox{~~and~~} P(A_n) = \prod_{k = 1}^\ell p^{\varepsilon_k} (1 - p)^{1 - \varepsilon_k}
\]
We want to apply the Borel-Canterelli Lemma however the $A_n$ are not disjoint, however the $A_{\ell n}$ are disjoint. Then the lemma tells us the monkey will write infinetely many times the works of Shakespeare.
\end{remark}

\section{A criterion for recurrence.}

Let $(\Omega, \mathcal{F}, P)$ a probability space. Let $(X_n)_{n \in \mathbb{N}^*}$ be a sequence of i.i.d. random variables with values in $\mathbb{Z}^d$ and let us set:
\[
S_0 = 0, \quad \forall n \geq 1, S_n = \sum_{i = 1}^{n} X_i
\]

\begin{proposition}
We have equivalence between:
\begin{enumerate}
\item $\sum_{n \geq 0} P(S_n = 0) = +\infty$
\item $P(\exists n \in \mathbb{N}^*, S_n = 0) = 1$
\item $P(S_n = 0 \mbox{~~infinetly often}) = 1$. 
\end{enumerate}
\end{proposition}

\begin{proof}

Let $T$ be the time of the first return to $\vb{0}$. Then:
\[
T = \inf \{ n \geq 1 : S_n = 0\} \mbox{~~where by convention~~} \inf \emptyset = +\infty
\]
Then let us define for $n\geq 1$:
\[
u_n = P(S_n = 0), \quad f_n = P(T = n)
\]
We have that for $n \geq 1$:
\[
u_n = P(S_n = 0) = P(S_n = 0, T \leq n) = \sum_{k = 1}^n P(S_n = 0, T = k) = \sum_{k = 1}^n P(T = k, S_n - S_k = 0)
\]
Now the two event $\{T = k\}$ and $\{S_n - S_k = 0\}$ are independent since they belong to two different $\sigma$-algebras: $\sigma(X_1, \cdots, X_k)$ and $\sigma(X_{k+1}, \cdots, X_n)$. The from the block grouping theorem we can write:
\[
u_n = \sum_{k = 1}^n P(T = k) P(S_n - S_k = 0) = \sum_{k = 1}^n P(T = k) P(S_{n - k} = 0) = \sum_{k = 1}^n f_k u_{n - k} = \sum_{k = 0}^n f_k u_{n-k}
\]
We introduce the generating functions of the sequences which are:
\[
U(s) = \sum_{n \geq 0} u_n s^n \mbox{~~and~~} F(s) = \sum_{n \geq 0} f_n s^n
\]
Then we have:
\[
U(s) - 1 = \sum_{n \geq 1} u_n s^n = \sum_{n \geq 1} \left(\sum_{k = 0}^n f_k s^k u_{n - k} s^{n - k}\right) = U(s)F(S)
\]
In the last equality we recognized the Cauchy product of the two series. Then these series are true for any $|s| < 1$. Then we can re-write the above as:
\[
\forall |s| < 1, U(s) (1 - F(s)) = 1
\] 
Then taking the limit as $s$ goes to 1 gives us that:
\begin{align*}
U(1) (1 - F(1)) = 1 &\Leftrightarrow \left( \sum_{n \geq 0} P(S_n = 0) \right) \left(1 - \sum_{n \geq 1} P(T = n)\right) = 1 \\
&\Leftrightarrow \left( \sum_{n \geq 0} P(S_n = 0) \right) \left(1 - P(\exists n \in \mathbb{N}^*, S_n = 0) \right) = 1
\end{align*}
Thus we have that:
\[
\sum_{n \geq 0} P(S_n = 0) = +\infty \Leftrightarrow P(\exists n \in \mathbb{N}^*, S_n = 0) = 1
\]
\end{proof}

\section{Random walk on $\mathbb{Z}$}
Let $(X_n)$ be a sequence of Bernoulli random variables where $(X_n = -1) = 1- p$ and $P(X_n = 1) = p$. Now we saw that:
\[
P(S_{2n} = 0) = \binom{2n}{n} p^n (1 - p)^n \stackrel{n\to \infty}{\sim} \frac{[4 p(1- p)]^n}{\sqrt{\pi n}}
\]
Hence if $p\neq\frac{1}{2}$ the series is converging and therefore the walk is transient, if $p = \frac{1}{2}$ the series diverges and hence the walk is recurrent. 

\section{Random walk on $\mathbb{Z}^2$.}
We take $(X_n)$ i.i.d. random values in $\mathbb{Z}^2$. Then we start by considering the simplest symmetric random walk:
\[
P(X_n = (1, 0)) = P(X_n = (0, 1)) = P(X_n = (-1, 0)) = P(X_n = (0, -1)) = \frac{1}{4}
\]
Let us compute:
\[
P(S_{2n} = 0) = \sum_{u, d, \ell, r, u = d, \ell = r, u + d + \ell , r = 2n} \frac{1}{4^{2n}} \binom{2n}{u, d, \ell, r} = \sum_{u, \ell, u+\ell = n} \frac{1}{4^{2n}} \binom{2n}{u, u, \ell, \ell} = \sum_{u = 0}^n \frac{1}{4^{2n}} \frac{(2n)!}{n!^2} \left(\frac{n!}{u!(n-u)!}\right)^2 
\]
Now up to a rewriting we get:
\[
P(S_{2n} = 0) = \frac{1}{4^{2n}} \binom{2n}{n} \sum_{u = 0}^n \binom{n}{u} \binom{n}{n - u} = \frac{1}{4^{2n}} \binom{2n}{n} \binom{2n}{n} = P(S_{2n} = 0 \mbox{~~in one dimension})^2 \stackrel{n \to \infty}{\sim} = \frac{1}{\pi n}
\]
Hence in 2D the symmetric walk is recurrent. 

\section{Random walk in $\Z^3$} (rivedi)
$(X_n)$ such that $P\bigg(X_n=\begin{pmatrix}
1\\ 0\\ 0
\end{pmatrix}\bigg)=\ldots=P\bigg(X_n=\begin{pmatrix}
0\\ 0\\ -1
\end{pmatrix}\bigg)=\frac{1}{6}$. Then we have
$$P(S_{2n}=0)=\sum_{i,j,k;\,\,i+j+k=n}\bigg(\frac{1}{6}\bigg)^{2n}\frac{(2n)!}{i!i!j!j!k!k!}$$
where $i$ number of upward and downward steps, (1,0,0) and (-1,0,0),... Then notice that $i!j!k!$ is smallest when $i=j=k$. In particular $i+j+k=n\Rightarrow i!j!k!\geq \bigg(\frac{n}{3})\bigg)^3$. Hence we get
$$P(S_{2n}=0)\leq \sum_{i,j,k;\,i+j+k=n}\bigg(\frac{1}{6}\bigg)^{2n}.....$$
$$\leq \bigg(\frac{1}{6}\bigg)^{2n}\binom{2n}{n}\frac{n!}{(n/3)!^3}\sim_{n\to\infty} \frac{m}{n^{3/2}}( stirling )$$
whence it follows
$$\sum P(S_{2n}=0)<\infty$$ and $(S_n)$ is transient. A result was proven by Montrell:
$$U_3=\frac{3}{(2\pi)^3}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\frac{dxdydz}{3-\cos(x)-\cos(y)-\cos(z)}=\frac{\sqrt{6}^3}{32\pi^3}\Gamma(1/24(\Gamma(5/25)\Gamma(7/24)\Gamma(11/24)$$

\section{Kolmogorov zero-one-law}
Let $(X_n)$ a sequence of random variables on $(\Omega,\mathcal{F},P)$. The tail or asymptotic $\sigma-$field of $(X_n)$ is 
$$\mathcal{J}=\bigcap_{n\geq 0}\sigma(X_n,X_{n+1},\ldots)$$
"An event is in the tail $\sigma$-field if, for any n, it does not depent on $X_0,\ldots,X_n$."
\begin{theorem}
Let $(X_n)$ be a sequence of independent random variables. If $A$ is an asymptotic event for $(X_n)_{n\in\N}$, i.e., $A\in\mathcal{J}$, then $P(A)=0$ or $P(A)=1$. 
\end{theorem}

\section{Application to the random walk on $\Z$}
$(X_n)$ such that $P(X_n=-1)=P(X_n=1)=\frac{1}{2}$. I want to prove that $P(S_n=0\,\,i.o)=1$. Be careul: $S_n=0$ i.o. is not asymptotic for $(X_n)$, it is asymptotic for $(S_n)$, but $(S_n)_n$ are not independent! 
\proof Let $M>0$
$$P(|S_n|\leq M)=\sum_{|k|\leq M}P(S_n=k)\leq (2M+1)P(S_n=0)\sim\frac{2M+1}{\sqrt{\pi n}}\to 0.$$
Then
$$P(\forall n, |S_n|\leq M)=0\,\,\,\,i.e.\,\,\,\, P((S_n)\text{ is bounded })=0$$
$$P(-\infty<\liminf_n S_n\leq \limsup_n S_n<\infty)=0$$
whence
$$P(\liminf S_n=-\infty,+\infty\text{ or }\limsup S_n=-\infty,+\infty)=1$$
then we use the trick: $\{\liminf S_n=-\infty\text{ or }+\infty\}$ is a tail event for $(X_n)$. By the 0-1 law, these events have probability 0 or 1, and at least one of them must have proba 1. $P(\liminf S_n=+\infty)=1\Rightarrow P(\limsup S_n=+\infty)$ yet $\limsup(S_n)=^{(d)}\limsup(-S_n)=-\liminf S_n=-\infty$.
Only possibility: $\liminf S_n=-\infty$ and $\limsup S_n=+\infty$. 

\chapter{The law of large numbers.}

\section{The Bienaymé-Chebyshev inequality.}

\begin{theorem}[Markov inequality]
Let $X$ be a real valued random variable on $(\Omega, \mathcal{F}, P)$. Then:
\[
\forall t \geq 0, P(|X| \geq t) \leq \frac{1}{t} E\big[|X|\big]
\]
\end{theorem}

\begin{proof}

We know that:
\[
E\big[|X|\big] = \int |X| \dd P = \int_{|X| < t} |X| \dd P + \int_{|X| \geq t} |X| \dd P \geq \int_{|X| \geq t} |X| \dd P \geq \int_{|X| \geq t} t \dd P = t P(|X| \geq t)
\]

\end{proof}

\begin{theorem}[Bienaymé-Chebyshev inequality]
Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $X$ be a real valued random variable on $(\Omega, \mathcal{F}, P)$, which admits a moment of order 2 i.e. $E[|X|^2] < +\infty$. Let us set $m = E[X]$ and $\sigma^2 = \text{Var}[X]$. Then the inequality is given by:
\[
\forall t \geq 0, P(|X - m| \geq t) \leq \frac{\sigma^2}{t^2}
\]
\end{theorem}

\begin{proof}
We start from the left hand side and use the Markov inequality:
\[
P(|X - m| \geq t) = P\left( (X - m)^2 \geq t^2 \right) \geq \frac{1}{t^2} E\left[ (X - m)^2 \right] = \frac{1}{t^2} \text{Var}[X]
\]
\end{proof}

\section{The weak law of large numbers.}

\begin{theorem}[Weak law of large numbers]
Let $\mu$ be a probability distribution on $\mathbb{R}$ which is integrable, i.e., $\int |x| \dd \mu(x) < +\infty$. Let us set $m = \int x \dd \mu(x)$ and let, for $n \geq 1$, $X_1, \cdots, X_n$ be $n$ independent and identically distributed random bariabls with law $\mu$ then:
\[
\forall \varepsilon > 0, \quad \lim_{n \to +\infty} P\left[ \left|\frac{X_1 + \cdots + X_n}{n} - m \right| > \varepsilon \right] = 0
\]
\end{theorem}

\begin{remark}
Notice that we have:
\[
P\left[ \left|\frac{X_1 + \cdots + X_n}{n} - m \right| > \varepsilon \right] = \mu^{\otimes n} \left( \left\{ \left( x_1, \cdots, x_n \right) : \left| \frac{x_1 + \cdots + x_n}{n} - m \right| > \varepsilon \right\} \right)
\]
Thus the weak law does not really involve an infinite sequence of independent identically distributed random variables, rather it is an asymptotic statement on the sequence of probability distributions $(\mu^{\otimes n})_{n \geq 1}$. 
\end{remark}

\begin{proof}

We first suppose that $\mu$ admits a moment of order 2, then we write:
\[
P\left[ \left|\frac{X_1 + \cdots + X_n}{n} - m \right| > \varepsilon \right] \leq \frac{1}{\varepsilon^2} \text{Var}\left[ \frac{X_1 + \cdots + X_n}{n} - m \right] \leq \frac{1}{\varepsilon^2} \text{Var}\left[ \frac{X_1 + \cdots + X_n}{n}\right] \leq \frac{1}{\varepsilon^2 n^2} \text{Var}\left[X_1 + \cdots + X_n \right]
\]
Now using the fact that the variables are independent we can say that the variance of the sum is the sum of the variance hence:
\[
P\left[ \left|\frac{X_1 + \cdots + X_n}{n} - m \right| > \varepsilon \right] \leq  \frac{1}{\varepsilon^2 n^2} (\text{Var}[X_1] + \cdots + \text{Var}[X_n]) = \frac{1}{\varepsilon^2 n^2} n \text{Var}[X_1] = \frac{\text{Var}[X_1]}{\varepsilon^2 n} \stackrel{n \to \infty}{\longrightarrow} 0
\]
Some extra work is needed to generalize to when the law is only integrable and not necessarily has a moment of order 2. However we will not do it here since it will be done in the next section during the proof of the strong law of large numbers.
\end{proof}

\section{The strong law of large numbers.}

\begin{theorem}[Strong law of large numbers]
Let $(X_n)_{n \geq 1}$ be a sequence of independent identically distributed random variables which admit a moment of order 1 i.e. $E[|X_1|] < +\infty$. Then the strong law of large numbers states that:
\[
\frac{X_1 + \cdots + X_n}{n} \stackrel{a.s.}{\longrightarrow} E[X_1]
\]
\end{theorem}

\begin{proof}

The first step consists in centering the variables. We replace $X_n$ by $X'_n = X_n - m$ where $m = E[X_1]$. Now we need to prove the following modified statement:
\[
\frac{X'_1 + \cdots + X'_n}{n} = \frac{X_1 + \cdots + X_n}{n} - m \stackrel{a.s.}{\longrightarrow} 0
\]
However now in order to continue with the proof we need to introduce a few more concepts. Which we will do now.
\end{proof}

\section{A maximal inequality}

\begin{theorem}[Maximal inequality]

Consider $(X_1, \cdots, X_n)$ to be $n$ independent centered random variables and we let $S_k = X_1 + \cdots + X_k$ for $k \in \llbracket 1, n \rrbracket$. Then we have:
\[
\forall \alpha \geq 0, \quad P(\max_{1 \leq k \leq n} |S_k| \geq \alpha) \leq \frac{1}{\alpha^2} E[S_n^2]
\] 

\end{theorem}

\begin{proof}

Let us set for $k \in \llbracket 1, n \rrbracket$, $A_k = \{ |S_k| \geq \alpha, S_1 < \alpha, \cdots, |S_{k-1}| < \alpha\}$. Then we have that:
\[
E[S_n^2] = \int S_n^2 \dd P \geq \int_{\bigcup_{k = 1}^n A_k} S_n^2 \dd P = \sum_{k = 1}^n \int_{A_k} S_n^2 \dd P 
\]
However now notice that $A_k$ depends only on $S_1, \cdots, S_k$ and hence we get the intuition of introducing $S_k$ to generate two independent events and we write:
\begin{align*}
\int_{A_k} S_n^2 \dd P &= \int_{A_k} (S_n - S_k + S_k)^2 \dd P = \int_{A_k} (S_n - S_k)^2 \dd P + 2 \int_{A_k} (S_n - S_k) S_k \dd P + \int_{A_k} S_k^2 \dd P\\
&\geq 0 + 2 E\left[ (S_n - S_k) S_k 1_{A_k} \right] + \int_{A_k} S_k^2 \dd P 
\end{align*}
Now using the independence of the two terms in the expectancy we can split it and we get:
\[
2 E\left[ (S_n - S_k) S_k 1_{A_k} \right] = 2 E\left[ (S_n - S_k) \right]E \left[ S_k 1_{A_k} \right] = 0 
\]
So we have:
\[
E[S_n^2] \geq \sum_{k = 1}^n \int_{A_k} S_k^2 \dd P \geq \sum_{k = 1}^n \int_{A_k} \alpha^2 \dd P = \sum_{k = 1}^n \alpha^2 P(A_k) = \alpha^2 P(\bigcup_{k = 1}^n A_k) = \alpha^2 P(\max_{1 \leq k \leq n} |S_k| \geq \alpha)
\]
\end{proof}

\section{Kolmogorov's criterion.}

\begin{theorem}[Kolmogorov's criterion]
Let $(X_n)$ be a sequence of centered independent random variables. If the series $\sum_{n \geq 1} \frac{1}{n^2} E[X_n^2]$ converges then:
\[
\frac{X_1 + \cdots + X_n}{n} \stackrel{a.s.}\longrightarrow 0
\]
\end{theorem}

\begin{proof}
Let $\varepsilon > 0$ and let us introduce the event:
\[
B_k = \{ \exists n \in \{2^k, \cdots, 2^{k+1} - 1\}, |S_n| > n \varepsilon\}
\]
We now estimate the probability of this event:
\[
P(B_k) \leq P(\exists n \in \{2^k, \cdots, 2^{k+1} - 1\}, |S_n| > 2^k \varepsilon) = P(\max_{2^k \leq n < 2^{k+1}} |S_n| > 2^k \varepsilon) \leq P(\max_{1 \leq n \leq 2^{k + 1}} |S_n| > 2^k \varepsilon)
\]
Now using the maximal inequality we can bound this from above as:
\[
P(B_k) \leq \frac{1}{2^{2k} \varepsilon^2} E[S_{2^{k+1}}^2] = \frac{1}{2^{2k} \varepsilon^2} \sum_{n = 1}^{2^{2k + 1}} E[X_n^2]
\]
Hence we have:
\[
\sum_{k \geq 1} P(B_k) \leq \sum_{k \geq 1} \frac{1}{2^{2k} \varepsilon^2} \sum_{n = 1}^{2^{2k + 1}} E[X_n^2] = \sum_{n \geq 1} \sum_{k : 2^{k + 1} \geq n} \frac{1}{2^{2k} \varepsilon^2} E[X_n^2] = \sum_{n \geq 1} \frac{1}{\varepsilon^2} E[X_n^2] \sum_{k : 2^{k+1} \geq n} \frac{1}{2^{2k}}
\]
Now the second sum gives:
\[
\sum_{k : 2^{k+1} \geq n} \frac{1}{2^{2k}} = \sum_{k \geq K} \frac{1}{2^{2k}} = \frac{\frac{1}{2^{2K}}}{1 - \frac{1}{2^2}} = \frac{4}{3 \cdot 2^{2K}} = \frac{16}{3 \cdot 2^{2(K+1)}} \leq \frac{16}{3 n^2}
\]
Thus:
\[
\sum_{k \geq 1} P(B_k) \leq \frac{16}{3 \varepsilon^2} \sum_{n \geq 1} E[X_n]^2 \leq \frac{16}{3 \varepsilon^2} M \mbox{~~by hypothesis.}
\]
Hence from the Borel-Cantelli lemmma part 1 we now that:
\[
P(B_k \mbox{~~occurs finitely often~~}) = 0
\]
Hence the complement has probability 1 or in other words:
\[
P(B_k \mbox{~~occurs finitely many times}) = 1
\]
Which can be re-written as:
\[
P(\exists K, \forall k \geq K, \forall n \in \{ 2^k, \cdots, 2^{k+1} - 1\}, |S_n| \leq n\varepsilon) = 1
\]
Or in other words:
\[
\forall \varepsilon > 0, P(\exists N, \forall n \geq N, |S_n| \leq n\varepsilon) = 1
\]
However we still have a small problem which is that the $\forall \varepsilon > 0$ should be inside the probability and not outside. In order to put it inside we simply can choose $\varepsilon \in \mathbb{Q}$ and then the countable intersection of events of probability 1 has probability 1 hence:
\[
P(\forall \varepsilon > 0, \varepsilon \in\mathbb{Q}, \exists N, \forall n \geq N, |S_n| \leq n \varepsilon) = 1
\]
\end{proof}

\section{Truncature.}
We truncate the variables $(X_n)$ and we defined a sequence $(Y_n)$ by setting:
\[
\forall k \geq 1, Y_k = \begin{cases} X_k &\mbox{~~if~~} |X_k| \leq k\\ 0 &\mbox{~~otherwise}\end{cases}
\]
Now we claim that $P(X_k \neq Y_k \mbox{~~infinitely often}) = 0$. Now we look at:
\[
P(X_k \neq Y_k) = P(|X_k| > k) = P(|X_1| > k)
\]
Then we have that:
\[
\sum_{k \geq 1} P(X_k \neq Y_k) = \sum_{k \geq 1} P(|X_1| > k) = \sum_{k \geq 1} \int_{k -1}^k P(|X_1| > k) \dd t \leq \sum_{k \geq 1} \int_{k - 1}^k P(|X_1| > t) \dd t = \int_0^{\infty} P(|X_1| > t) \dd t = E[|X_1|]
\]
Thus by B.C. we know that $P(X_k \neq Y_k \mbox{~~i.o.}) = 0$. Now we set for $n \geq 1$, $Z_n = Y_n - E[Y_n]$ and we write:
\[
\frac{X_1 + \cdots + X_n}{n} = \frac{(X_1 - Y_1) + \cdots + (X_n - Y_n)}{n} + \frac{Z_1 + \cdots + Z_n}{n} + \frac{E[Y_1] + \cdots + E[Y_n]}{n}
\]
Now we know that the first term goes to 0 almost surely since there are only finitely many $X_k - Y_k$ which are non-zero. Then we have that:
\[
E[Y_n] = \int Y_n \dd P = \int_{|X_n| \leq n} X_n \dd P = E[X_n 1_{|X_n| \leq n} = E[X_1 1_{|X_1| \leq n}] \stackrel{n \to \infty}{\longrightarrow} E[X_1]
\]
Hence we have (from Cesaro) that:
\[
\frac{E[Y_1] + \cdots + E[Y_n]}{n} \stackrel{n \to \infty}{\longrightarrow} E[X_1]
\]
It remains to prove that $\frac{Z_1 + \cdots Z_n}{n} \stackrel{a.s.}\longrightarrow 0$. We will use Kolmogorov's criterion for this. $Z_1, \cdots, Z_n$ are independent and centered from definition. Hence to apply Kolmogorov we need to show that $\sum_{n \geq 1} \frac{E[Z_n^2]}{n^2} < \infty$. Now notice that:
\[
E[Z_n^2] = E[(Y_n - E[Y_n])^2) = \text{Var}(Y_n) \leq E[Y_n^2]
\]
Then we have:
\[
E[Y_n^2] = \int_{|X_n| \leq n} X_n^2 
\]
Hence we have:
\[
\sum_{n \geq 1} \frac{E[Z_n^2]}{n^2} \leq \sum_{n \geq 1} \frac{E[Y_n^2]}{n^2} \leq \sum_{n \geq 1} \frac{1}{n^2} \int_{|X_n| \leq n} X_n^2 \dd P = \sum_{n \geq 1} \frac{1}{n^2} \sum_{i = 1}^n \int_{i - 1 < |X_1| \leq i} X_1^2 \dd P \leq \sum_{i \geq 1} (\sum_{n \geq i} \frac{1}{n^2}) \int_{i - 1 < |X_1| \leq i} X_1^2 \dd P 
\]
Now the sum simplifies to:
\[
\sum_{n \geq i} \frac{1}{n^2} = \frac{1}{i^2} + \sum_{n \geq i + 1} \frac{1}{n^2} \leq \frac{1}{i^2} + \sum_{n \geq i+1} \int_{n - 1}^n \frac{\dd t}{t^2}\leq \frac{1}{i^2} + \int_i^{+\infty} \frac{\dd t}{t^2} \leq \frac{1}{i^2} + \frac{1}{i} \leq \frac{2}{i}
\]
Hence plugging it back on top we obtain:
\[
\sum_{n \geq 1} \frac{1}{n^2} E[Z_n^2] \leq \sum_{i \geq 1} \frac{2}{i} \int_{i - 1< |X_1| \leq i} X_1^2 \dd P \leq \sum_{i \geq 1} \frac{2}{i} \int_{i - 1 | X_1| \leq i} i |X_1| \dd P = 2 \int |X_1| \dd P = 2E[|X_1|] < \infty
\]
Hence we can conclude with the help of the Kolmogorov criterion.

\section{Discussion}

We have 2 laws of large numbers:
\[
\frac{X_1 + \cdots + X_n}{n} \stackrel{P}{\longrightarrow} E[X_1] \mbox{~~and~~} \frac{X_1 + \cdots + X_n}{n} \stackrel{a.s.}{\longrightarrow} E[X_1]
\]
We can immediately see that the strong law implies the weak law, so why do we keep the weak law ? The weak law does not require the construction of an infinite family of random variables while the strong one does. Furthermore the strong law provides almost no new information with respect to the weak law and hence it amounts simply to a lot of work for little reward.

\chapter{Central limit theorem}

\section{Convergence in distribution (or in Law)}
There are sequences of random variables that don't converge in any of the previously mentioned types of convergence, but whose law converges. For example consider $(\Omega,\mathcal{F},P)=([0,1],\mathcal{B}([0,1]),\lambda)$ and law of $X_n:$Bernoulli(1/2) converges in law but not $a.a$ or $L^1$ or $P$. 

\begin{definition}
The sequence $(X_n)_n$ of random variables converges in distribution or in law towards the random varibale $X$ if and only if: for any continuous bounded function $f:\R\to\R$ we have:
$$\lim_{n\to\infty}E(f(X_n))=E(f(X))$$
\end{definition}

\begin{remark}
Remark that $E(f(X_n))=\int f(X_n)dP=\int_{\R}f(x)dP_{X_n}(x)$ involves only the law of $X_n$. The convergence in distribution involves only the laws of the random variables $(X_n)_n$. It makes sense even if the random varibales are not defined on the same probability space.
\end{remark}

\begin{notation}
This convergence is denoted: $X_n\xrightarrow[]{\mathcal{D}} X$ or $X_n \xrightarrow[]{\mathcal{l}} X$.
\end{notation}

\begin{proposition}
$X_n\xrightarrow[]{\mathcal{D}} X$ if and only if for any function $f:\R\to\R$ continuous with compact support, we have
$$\lim_{n\to\infty} E(f(X_n))=E(f(X))$$
\end{proposition}
\proof $\Rightarrow$ is obvious.\\
$\Leftarrow$: suppose that for any function $f:\R\to\R$ continuous, compact support, $E(f(X_n))\to E(f(X))$ Let $f:\R\to\R$ be a continuous bounded function. Let $\epsilon>0$. I claim that $\exists M>0$, $P(X\in[-M,M])\geq 1-\epsilon$. In fact, $P(X\in\R)=1$ and $P(X\in\R)=P(\cup_{M>0}\{X\in[-M,M]\})=\lim_{M\to\infty}P(X\in[-M,M])$. Now, let $\phi$ be the function given by straight lines between $-M-1$ and $-M$ and $M$ and $M+1$ and constant in the middle. $\phi$ is continuous, bounded support and $1_{[-M,M]}\leq \phi\leq 1_{[-M-1,M+1]}$. $\phi$ has compact support , we have $\lim_{n\to\infty}E(\phi(X_n))=E(\phi(X))\geq P(X\in[-M,M])\geq 1-\epsilon$ so $\exists N$, $\forall n\geq N$, $E(\phi(X_n))\geq 1-2\epsilon$. Now, let $f:\R\to\R$ be a continuous and bounded, we have 
$$E(f(X_n))-E(f(x))=E(f(X_n))-E(f\phi(X_n))+E(f\phi(X_n))-E(f\phi(X))+E(f\phi(X))-E(f(X))$$
whence 
\begin{align*}
|E(f(X_n))-E(f(X))|&\leq E(|f|(1-\phi)(X_n))+|E(f\phi(X_n))-E(f\phi(X))|+E(|f|(1-\phi)(X)|\\
&\leq ||f||_{\infty}(1-E(\phi(X_n))+|E(f\phi(X_n))-E(f\phi(X))|+||f||_{\infty}(1-E(\phi(X))\\
&\leq ||f||_{\infty}(2\epsilon+\epsilon)+|E(f\phi(X_n))-E(f\phi(X))|
\end{align*}
Yet $f\phi$ has compact support, so $E(f\phi(X_n))\to E(f\phi(X))$. $\exists N'$, $\forall n\geq N'$, $|E(f\phi(X_n))-E(f\phi(X))|<\epsilon$. Thus, for $n\geq max(N.N')$, $|E(f(X_n))-E(f(X))|\leq ||f||_{\infty}3\epsilon+\epsilon$

\section{Relationship with the other convergences} 
\begin{proposition}
If $X_n\xrightarrow[]{P} X$, then $X_n\xrightarrow[]{\mathcal{D}} X$.
\end{proposition}
\proof Let $f:\R\to\R$ continuous, compact support. Then $f$ is i nfact uniformly continuous:
$$\forall\epsilon>0,\exists\delta>0,\forall x,y\in\R, |x-y|<\delta\Rightarrow |f(x)-f(y)|<\epsilon$$ 
We rewrite
\begin{align*}
&E(f(X_n))-E(f(X))=\int_{\Omega}(f(X_n)-f(X))dP=\int_{|X_n-X|<\delta}(f(X_n)-f(X))dP+\int_{|X_n-X|\geq \delta}(f(X_n)-f(X))dP\\
&\Rightarrow |E(f(X_n))-E(f(X))|\leq \epsilon+||f||_{\infty}P(|X_n-X|\geq \delta)
\end{align*}
Now $X_n\xrightarrow[]{P} X$. So $\exists N$, $\forall n\geq N$, $P(|X_n-X|\geq \delta)<\epsilon/||f||_{\infty}$. Whence, for $n\geq N$, $|E(f(X_n))-f(X)|\leq \epsilon+\epsilon=2\epsilon$.\\
\begin{remark}
Summary: $L^2\Rightarrow L^1\Rightarrow P\Rightarrow \mathcal{D}$ and $a.s\Rightarrow P$.
\end{remark}

\section{Characteristic functions}
Let $\mu$ be a probability measure on $\R$. Its characteristic fucntion is the function $\varphi:\R\to\mathbb{C}$ defined by $\forall u\in\R$, $\varphi(u)=\int_{\R}e^{iux}d\mu(x)$. Let $X$ be a real valued random variable. Its characteristic function is the function $\varphi_X:\R\to\mathbb{C}$ defined by $\forall u\in\R$, $\varphi_X(u)=E(e^{iuX})$. In fact, $E(e^{iuX})=\int e^{iuX}dP=\int_{\R}e^{iux}dP_X(x)$. The characteristic function of $X$ is the Fourier tranform of its law $P_X$. Notice that $\varphi(0)=1$, $\varphi(-u)=\varphi(u)$, $\varphi$ is continuous. 

\section{Moments}
\begin{theorem}
Let $X$ be a random variable which admits a moment of order $k\geq 1$. Then $\varphi_X$ is $k$ times differentiable at $0$ and moreover
$$\varphi_X^{(k)}(0)=i^kE(X^k)$$
\end{theorem}
\proof $k=1$. Suppose $E(|X|)<\infty$. $\varphi_X(u)=\int_{\R}e^{iux}dP_X(x)$, then $\partial_u e^{iux}=ixe^{iux}$. Then $|\partial_ue^{iux}|\leq |x|$ (uniformly wrt $u$, integrable wrt $P_X$). Use a result on $\int$ depending on a parameter to conclude
$$\varphi'_X(u)=\int ixe^{iux}dP_X(x)$$

\section{Gaussian variables}
A variable $X$ has a distribution which is gaussian with mean $m$ and variance $\sigma^2$ if its law has density
$$\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\frac{(x-m)^2}{\sigma^2}}$$
with respect to the Lebesgue measure. We denote this law by $\mathcal{N}(m,\sigma^2)$. Suppose $X\sim\mathcal{N}(m,\sigma^2)$ and let us compute $\varphi_X$:
\begin{align*}
\varphi_X(u&)=E(e^{iux})=\int_{\R}e^{iux}e^{-\frac{1}{2}\frac{(x-m)^2}{\sigma^2}}\frac{dx}{\sqrt{2\pi}\sigma}\,\,\,(y=\frac{x-m}{\sigma}, x=\sigma y+m)\\
&=\int e^{iu\sigma y}e^{ium}e^{-\frac{1}{2}y^2}\frac{dy}{\sqrt{2\pi}}\\
&=\frac{e^{ium}}{\sqrt{2\pi}}\int_{\R}e^{-\frac{1}{2}(y-iu\sigma)^2-\frac{\sigma^2 u^2}{2}}dy=\frac{1}{\sqrt{2\pi}}e^{ium-\frac{\sigma^2 u^2}{2}}\int_{\R}e^{-\frac{1}{2}(y-iu\sigma)^2}dy
\end{align*}
we apply the Residue Theorem to the function $e^{-\frac{z^2}{2}}$ (entire function) and the contour $\gamma_R$ (rettangolo tra $\R$ e $\R-iu\sigma$ di side lunghezza $R$). Hence we find
$$\int_{\gamma_R}e^{-\frac{1}{2}z^2}dz=0=\int_{-R}^Re^{-\frac{1}{2}t^2}dt+\int_{t=0}^{-u\sigma}e^{-\frac{1}{2}(R*it)^2}idt(\to_{R\to\infty} 0)+\int_{R}^{-R} e^{....}+...$$
so $\int_{\R-i\sigma u}e^{-\frac{1}{2}z^2}dz=\int_{\R}e^{-\frac{1}{2}z^2}dz=\sqrt{2\pi}=\frac{1}{\sqrt{2\pi}}e^{ium-\frac{\sigma^2 u^2}{2}}\int_{\R}e^{-\frac{1}{2}(y-iu\sigma)^2}dy$ thus $\varphi_X(u)=\exp(ium-\frac{\sigma^2u^2}{2})$. Let us denote $g_{\sigma}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\frac{x^2}{\sigma^2}}$. We have 
$$\int e^{iux}g_{\sigma}(x)dx=e^{-\sigma^2 u^2/2}=\frac{\sqrt{2\pi}}{\sigma}g_{1/\sigma}(u)$$
the cool thing of the gaussia is that it is an eigenvalue of the Fourier transform.


\section{Injectiveness}
\begin{theorem}
The map which associates to a probability measure on $\R$ its characteristic function is one to one. In other words, the characteristic function of a random variables $X$ characterizes its law.
\end{theorem}
\proof Let $X$ be a random variable, $\varphi_X$ its characteristic function. Let $Y$ be a random variable with distribution $\mathcal{N}(0,1)$, which is independent of $X$ and let us consider $X+\sigma Y$. I claim that $X+\sigma Y\xrightarrow[]{P} X$ as $\sigma \to 0$. Indeed $P(|X+\sigma Y-X|>\epsilon)=P(|\sigma Y|>\epsilon)=P(|Y|>\frac{\epsilon}{\sigma})\to_{\sigma\to 0} 0$. Let $f:\R\to\R$ continuous bounded, we have 
\begin{align*}
E(f(X+\sigma Y))&=\int\int f(x+\sigma y)dP_X(x)g_{\sigma}(y)dy\,\,\,\,(y\to \sigma y)\\
&=\int\int f(x+y)dP_X(x)g_{\sigma}(y)dy\,\,\,\,(z=x+y)\\
&=\int\int f(z)dP_X(x)g_{\sigma}(z-x)dz
\end{align*}
where we used that $g_{\sigma}(u)=\frac{1}{\sigma\sqrt{2\pi}}\int e^{iut}g_{1/\sigma}(t)dt$ and $\int e^{iut}g_{\sigma}(t)dt=\frac{\sqrt{2\pi}}{\sigma}g_{1/\sigma}(u)$. Hence 
\begin{align*}
E(f(X+\sigma Y))&=\int\int\int f(z)\frac{1}{\sigma\sqrt{2\pi}}e^{i(z-x)t}g_{1/\sigma}(t)dtdP_X(x)dz=_{Fubini}f(z)\bigg(\int_{x}e^{-ixt}dP_X(x)\bigg)g_{\/\sigma}(t)dtdz\\
&=\int\int f(z)\varphi_X(-t)g_{1/\sigma}(t)dtdz\frac{1}{\sigma\sqrt{2\pi}}
\end{align*}
Now, $X+\sigma Y\xrightarrow[]{P} X$ so $X+\sigma Y\xrightarrow[]{\mathcal{D}} X$, thus 
$$E(f(X))=\lim_{\sigma\to 0}E(f(X+\sigma Y))=\lim_{\sigma\to 0}\int\int f(z)\varphi_X(-t)g_{1/\sigma}(t)dtdz$$
and thus $E(f(X))$ is completely determined by $\varphi_X$.

\section{Paul Levy's theorem}
Let $(X_n)_n$ be a sequence of random variables. Let $X$ be a random variable. Let us denote by $(\varphi_{X_n})$, $\varphi_X$ the associated characteristic functions.  
\begin{theorem}(Paul Levy, weak form)
We have equivalence between:
\begin{enumerate}
\item $X_n\xrightarrow[]{\mathcal{D}} X$
\item $\forall u\in\R$, $\lim_{n\to\infty}\varphi_{X_n}(u)=\varphi_X(u)$.
\end{enumerate}
\end{theorem}
\proof (1) implies (2) is easy because $f(x)=e^{iux}$ is continuous and bounded!\\
(2) implies (1): let $f:\R\to \R$ continouous compact support. Let $\sigma >0$ and let $Y$ be a random variable, independent of $X_n,X$, with distribution $\mathcal{N}(0,1)$. We write 
$$E(f(X_n))-E(f(X))=E(f(X_n)-E(f(X_n+\sigma Y))+E(f(X_n+\sigma Y))-E(f(X+\sigma Y))+E(f(X+\sigma Y))-E(f(X))$$
hence 
\begin{align*}|E(f(X_n))-E(f(X_n+\sigma Y))|&\leq \epsilon+||f||_{\infty}P(\sigma |Y|>\delta)\,\,\,\,(\text{uniform ocntinuity})\\
|E(f(X)))-E(f(X+\sigma Y))|&\leq "
\end{align*}
so 
\begin{align*}
|E(f(X_n))-E(f(X))|&\leq 2\epsilon +2||f||_{\infty}P(|Y|>\frac{\delta}{\sigma})+|E(f(X_n+\sigma Y))-E(f(X+\sigma Y))|
\end{align*}
Let $\sigma$ small enough so $||f||_{\infty}P(|Y|>\frac{\delta}{\sigma})<\epsilon$. For this $\sigma $ fixed, we have
$$|E(f(X_n))-E(f(X))|\leq 4\epsilon+|E(f(X_n+\sigma Y)-E(f(X+\sigma Y)|.$$
I claim that $E(f(X_n+\sigma Y))\to_{n\to\infty}E(f(X+\sigma Y))$ where we can use the before last result from the previous section.


\section{The central limit theorem} Let $(X_n)_n$ be a sequence of iid random variables which have a moment of order 2, i.e., $E(X_1^2)<\infty$. Let $m=E(X_1)$, $\sigma^2=Var(X_1)$, then 
$$\frac{X_1+\ldots +X_n-nm}{\sqrt{n}}\xrightarrow[]{\mathcal{D}}\mathcal{N}(0,\sigma^2)$$

\proof We center the variables by putting $X_i'=X_i-m$ and we set $Y_m=\frac{X_1'+\ldots+X_n'}{\sqrt{n}}$ and we compute $$\varphi_{Y_n}(u)=E(e^{iuY_n})=E(e^{iu\frac{X_1'+\ldots+X_n'}{\sqrt{n}}})=E(e^{iu\frac{X_1'}{\sqrt{n}}}\cdots e^{iu\frac{X_n'}{\sqrt{n}}}=E(e^{iu\frac{X_1'}{\sqrt{n}}})\cdots E(e^{iu\frac{X_n'}{\sqrt{n}}})$$
$$=E(e^{iu\frac{X_1'}{\sqrt{n}}})^n$$
where we used independence and same law for the $X_i'$. So $\varphi_{Y_n}(u)=\varphi_{X_1'}(\frac{u}{\sqrt{n}})^n$. Since $X_1'$ has a moment of order 2, $\varphi_{X_1'}$ is twice differentiable and $\varphi_{X_1'}(u)=1+iuE(X_1')-\frac{u^2}{2}E(X_1'^2)+o(u^2)_{u\to 0}$. Hence 
$\varphi_{Y_n}(u)=(\varphi_{X_1'}(\frac{u}{\sqrt{n}}))^n=(1-\frac{u^2}{2n}\sigma^2+o(1/n))^n\to_{n\to\infty}e^{-\frac{\sigma^2u^2}{2}}$ which is the characteristic function of $\mathcal{N}(0,\sigma^2)$. The conclusion follows from Paul Levy.
%

\chapter{Percolation}

\section{Birth of the model}

We consider a large volume of water and we immerse a sponge in the water. The sponge has a lot of small channels and we know that the size of the water molecule $\ll$ the diameter of the channel $\ll$ the diameter of the sponge. The question is now to know whether the water will reach the center of the stone or not. To study this question, and more generally, to study the structure of random porous media, Broadbent and Hammersly invented the percolation model in 1957. 

\section{The square lattice.}

\begin{definition}
A graph is a pair $(V, E)$ where $V$ is a set (the vertices) and $E$ is a set of edges on $V$, that is a collection of pairs $\{x, y\}$ of elements of $V$. 
\end{definition}

Now we endow $\mathbb{Z}^2$ with an infinite graph structure, the set $\mathbb{E}^2$ of edges is the set of pairs $\{x, y\}$ of points of $\mathbb{Z}^2$ which are nearest neighbors. That is $\mathbb{E}^2 = \{ \{x, y\} | x, y \in \mathbb{Z} \land |x - y| = 1\}$. 

\begin{definition}
A path in $(\mathbb{Z}^2, \mathbb{E}^2)$ is a sequence $x_0, x_1, \cdots, x_i, x_{i+1}, \cdots$ of \textbf{distinct} vertices such that for any $i \geq 0$, $\{x_i, x_{i+1}\} \in \mathbb{E}^2$. If the path terminates at $x_n$ for some $n$, we say that it is finite and has length $n$. Otherwise the path is infinite and we say that it connects $x_0$ to $\infty$. 
\end{definition}

\begin{definition}
A circuit is a finite path $x_0, x_1, \cdots, x_n$ such that $\{x_0, x_n\} \in \mathbb{E}^2$. Such a circuit has length $n+1$. 
\end{definition}

\section{The probability space.}

The edges will play the role of the channels in the stone. Hence they can be either open (the water can flow) or close (the water cannot flow) them. We shall choose in between the two cases randomly and independently with probability $p$. We consider the space $\Omega = \{ 0, 1 \}^{\mathbb{E}^2}$. An element of $\Omega$ is a map from $\mathbb{E}^2$ to $\{0, 1\}$. An edge $e$ is said to be open in the configuration $\omega$ if $\omega(e) = 1$ and closed if $\omega(e) = 0$. We now endow $\Omega$ with the $\sigma-$field $\mathcal{F}$ generated by the finite dimensional cylinders which are:
\[
\mathcal{F} = \{ \{ \omega \in \Omega : \omega(e_1) = \varepsilon_1, \cdots, \omega(e_n) = \varepsilon_n \} , n\geq 1, e_i \in \mathbb{E}^2, \varepsilon_i \in \{0, 1\} \}
\]  
We now endow the measurable space $(\Omega, \mathcal{F})$ with the probability $P_p$ which is the product of the Bernouilli measures with parameter $p$:
\[
P_p = \bigotimes_{e \in \mathbb{E}^2} ((1 - p)\delta_{\omega(e) = 0} + p \delta_{\omega(e) = 1})
\]
My final probability space on which all the action takes place is $(\Omega, \mathcal{F}, P_p)$. 

\section{The open clusters.}

We draw randomly the state of each edge and we remove the closed edges. We obtain a random graph. The goal of percolation theory is to understand the structure of this random graph. Let $\omega \in \Omega$ be a configuration. We consider the graph whose vertices are $\mathbb{Z}^2$ and whose edges are the edges which are open in $\omega$ only. An open cluster of $\omega$ is a connected component of the graph. The open connected component of $x$ is denoted by $C(x)$ and is called the open cluster of $x$. We wish to understand $C(x)$, its geometry, for instance its diameter, or its cardinality $|C(x)|$, and more generally, its law. Since the model is translationally invariant hence it is enough to focus on the cluster  of the origin $C(0)$.

\section{Percolation probability}
A fundamental quantity to analyze the model is the percolation probability defined by:
\[
\theta(p) = P_p(|C(0)| = \infty) = P_p( 0 \leftrightarrow \infty) = P_p(\mbox{there exists an infinite open path starting from 0})
\] 
Notice that this is not well defined as is hence we need to re-write it as:
\[
\theta(p) = P_p(0 \leftrightarrow \infty) = P_p\left(\bigcap_{n \in \mathbb{N}} \{ |C(0)| > n\}\right) 
\]
We now want to study $\theta$. 

\begin{proposition}
The map $p \in [0,1] \mapsto \theta(p)$ is non-decreasing. 
\end{proposition}

\begin{proof} (The coupling method) Let $p_1 < p_2$ in $[0, 1]$ and let $(\Omega_1, \mathcal{F}_1), (\Omega_2, \mathcal{F}_2)$ be two copies of $(\Omega, \mathcal{F})$. We consider the product space $(\Omega_1, \times \Omega_2, \mathcal{F}_1 \otimes \mathcal{F}_2)$ we shall build a probability $P$ on this space such that: 
\begin{itemize}
\item the law of the first projection $\omega_1$ is $P_{p_1}$.
\item the law of the second projection $\omega_2$ is $P_{p_2}$. 
\item Every edge which is open in $\omega_1$ is also open in $\omega_2$: $\forall e \in \mathbb{E}^2, \omega_1(e) \leq \omega_2(e)$. 
\end{itemize}
Suppose $P$ has been built then we write:
\[
\theta(p) = P_{P_1}(0 \leftrightarrow \infty) = P(0 \leftrightarrow \infty \mbox{~in~} \omega_1) \leq P(0 \leftrightarrow \infty \mbox{~in~} \omega_2) = P_{P_2}(0 \leftrightarrow \infty) = \theta(p_2)
\]
Now we consider $U(e), e \in \mathbb{E}^2$ be a collection of independent and identically distributed random variables with uniform distribution on $[0, 1]$, defined on some $(\Omega, \mathcal{F}, P)$. We set: $\omega_i(e) = 1_{\{U(e) < p_i\}}$. This correctly defines $P$ and we are done.

\end{proof}

\section{Phase transition.}
One of the main interests of the percolation model is that it is one of the simplest models that exhibits a phase transition.

\begin{theorem}

There exist a critical parameter $p_c$ such that $0 < p_c < 1$ and $\theta(p) = 0$ for $p < p_c$ and $\theta(p) > 0$ for $p > p_c$. 

\end{theorem}

\begin{proof}
We define $p_c = \sup \{ p \in [0, 1] : \theta(p) = 0 \}$. We have to prove that $p_c \in ]0, 1[$. Let us prove first that $p_c$ is positive or equivalently that $\theta(p) = 0$ for $p$ in the neighborhood of 0. We write:
\begin{align*}
\theta(p) &= P_p(0 \leftrightarrow \infty) \leq P_p(\mbox{there exists an open path of length~} n \mbox{~starting at the origin})\\
&= P_p\left( \bigcup_{\gamma : |\gamma| = n} \{\gamma \mbox{~open~}\} \right) \leq \sum_\gamma P_p(\gamma \mbox{~open~}) = \sum_\gamma p^n = p^n |\{\gamma : |\gamma| = n\}| \leq p^n \cdot 4 \cdot 3^{n-1} 
\end{align*}
Hence we have proved that:
\[
\forall n \geq 1, \theta(p) \leq \frac{4}{3} (3p)^n
\]
Then if $p < \frac{1}{3}$ we can immediately see that $\theta(p) = 0$ by taking the limit of the above expression. We now want to prove that $p_c < 1$. We consider now the notion of duality. The dual graph to $(\mathbb{Z}^2, \mathbb{E}^2)$ denoted by $(\mathbb{Z}^{2\star}, \mathbb{E}^{2\star})$ where $\mathbb{Z}^{2\star} = \mathbb{Z}^2 + (\frac{1}{2}, \frac{1}{2})$. Now to an edge $e$ of $\mathbb{E}^2$ we associate the dual edge $e^\star$ such that $e$ and $e^\star$ intersect orthogonally in their middle and we declare $e^\star$ to be open if and only if $e$ is open. Now it is a topological fact that if $C(0)$ is finite then there exists a dual circuit of closed edges surrounding $0$. Then we have that:
\begin{align*}
1 - \theta(p) &= P(|C(0)| < \infty) \leq P_p(\mbox{there exists a closed dual circuit surrounding 0}) \\
&\leq P_p \left( \bigcup_{\gamma^\star} \{\gamma^\star \mbox{~is closed}\} \right) \leq \sum_{\gamma^\star} (1 - p)^{|\gamma^\star|}  \leq \sum_{n \geq 4} (1 - p)^n n 4 \cdot 3^{n-1} \\
&= (1 - p)^2 \sum_{n \geq 4} (1 - p)^{n - 2} n 4\cdot  3^{n-1} \stackrel{n \to +\infty}{\longrightarrow} 0 
\end{align*}
\end{proof}

\section{Conjecture on $\theta(p)$}

Kesten in 1980 proved that $p_c = \frac{1}{2}$. We can also ask the same question in higher dimensions than $d = 2$. 


\end{document}